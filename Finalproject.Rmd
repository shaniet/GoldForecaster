---
title: "Final Project"
author: "Shanie Talor, Arif Abd Aziz, Jon Girolamo"
date: '2023-06-02'
output: pdf_document
---

# Introduction

(We don't have to polish this RMD in terms of labels or descriptions. Final presentation will be in powerpoint. We'll still submit this as the source code though so we should still be commenting what we're doing with code) Jon

# Load Data
```{r}
# Read csv
gold <- read.csv("gold.csv", sep = ",", header = TRUE)

# Omit all rows that have NA Values: 6 Rows: 05/31/1979 to 03/31/2023
gold <- na.omit(gold)

# Add Binary Classifier Variable
gold$Binary.PercChange <- ifelse(gold$PercChangeForc > 0.1, 1, 0) 

# Check if Features Are Numeric (Returns False, So Non-Numeric Variables)
all(sapply(gold, is.numeric))


# gold <- gold[,-26]
# gold <- gold[,-24]
# gold <- gold[,-2]
# gold <- gold[,-1]

# head(gold)
# Head Gold
head(gold)
```

We converted our percent change forecast variable into a binary classifier. The hurdle rate for our binary classifier ("Binary.PercChange") is set at 0.1%, so that edge cases that are very close to 0% (0%:0.0999%) are classified as non-investment opportunities. Percent changes in gold (month over month) greater than 0.1% will be considered "buy" opportunities, with a classifier value of 1. 

# Initial Analysis of Features 

In order to better understand our features, we'll first look at the histograms of our features as well as the scatterplots between our features and our main response variables: monthly percent changes in gold for our regression problem and our binary classiifer for our classification problem. 

### Histogram for Loops
```{r}
# Set Feature Space Excluding Lagged Variables and Other Miscellenous Feature 
library(dplyr)
library(ggplot2)
library(stats)
library(reshape2)
gold_histograms <- gold %>% select(-c("Date","PercChangeLag1","PercChangeLag2",
                                "PercChangeLag3", "PercChangeLag4",
                                "PercChangeLag5","Inf.L1","Inf.L2",
                                "Inf.L3","Inf.L4","FedFundsRateL1"))
gold_histograms <- as.data.frame(lapply(gold_histograms, as.numeric))
head(gold_histograms)

# For Loop for Histograms
for (feature in names(gold_histograms)){
  hist(gold_histograms[[feature]], xlab = paste0(feature), ylab = "Frequency",
       main = paste0("Histogram of ",feature))
}

# For Loop for Box Plots
for (feature in names(gold_histograms)){
  boxplot(gold_histograms[[feature]], ylab = paste0("Value of ",feature),
       main = paste0("Boxplot of ",feature))
}
```

The boxplots and histograms show that our variables are on average normally distributed. There are outliers in some of our features, such as percent changes in monthly prices and changes in bank reserves. However, we believe these outliers are still essential in predicting the movement of gold prices. 

### Correlation Values
```{r} 
# Data Clean for Scatter Plots 
set.seed(490782)
gold_cor <- gold %>% select(-c("Date","Average.Price")) 
gold_cor <- as.data.frame(lapply(gold_cor, as.numeric))
head(gold_cor)

# For Loop for Scatter Plots 
for (feature in names(gold_cor)[!names(gold_cor) %in% c("Binary.PercChange")]){
  plot(gold_cor[[feature]], gold_cor$PercChangeForc, type = "p",
       xlab = paste(feature), ylab = "Monthly Forecasted Change in Gold Prices",
       main = paste0("Monthly Changes in Gold Prices Against ",feature)) 
  line <- lm(gold_cor$PercChangeForc ~ gold_cor[[feature]])
  abline(line, col = "blue")
}

# Heat Map
cor_matrix <- round(cor(gold_cor),2)
melted_cor_gold <- melt(cor_matrix)
ggplot(data = melted_cor_gold, aes(x = Var1, y = Var2, fill = value))+
  geom_tile()+theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Our scatterplots and heat maps show that there are some pockets of correlation between features in our data set. Looking at our main response variables (PercChangeForc & Binary.PercChange), there are no visibly strong correlations between them and our various features. However, we believe a combination and enhancement of these features through the various algorithms we'll be looking at will allow us to best predict both response variables - in a regression and classification setting. 

# Initial Data Cleaning
```{r}
# Gold Data for Regression Problem:
head(gold)
gold.r <- gold %>% select(-c("Date","Average.Price","Binary.PercChange","Oil",
                             "PPIJewelry"))

# Gold Data for Classification Problem:
gold.c <- gold %>% select(-c("Date","Average.Price","PercChangeForc","Oil",
                             "PPIJewelry")) 

# Convert Data to Numeric
gold.r <- as.data.frame(lapply(gold.r, as.numeric)) 
gold.c <- as.data.frame(lapply(gold.c, as.numeric))

# Data Check & Numeric Check
all(sapply(gold.r, is.numeric))
all(sapply(gold.c, is.numeric))
head(gold.r)
head(gold.c)
```

# Feature Selection: Boruta Algorithm
```{r, echo = FALSE}
# Run Boruta  algorithm on clean data
library(Boruta)
library(randomForest)
boruta.gold <- Boruta(PercChangeForc~., data = gold.r, doTrace = 2, 
                       randomForest = TRUE)
```

### Plot Boruta Algorithm
```{r, echo = FALSE}
plot(boruta.gold, main = "Boruta Algorithm for Gold Pricing Algorithm")
```

# Create Five Fold Cross Validation
```{r}
library(caret)
set.seed(123)
folds <- createFolds(gold.r$PercChangeForc, k = 5, returnTrain = TRUE)
```

# Regression

### OLS (Jon)
```{r}
set.seed(123)
# Run linear regression
lin.reg <- lm(PercChangeForc~., data = gold.r)

summary(lin.reg)

# Create empty vector
lin.reg.mse <- vector()

# CV loop for linear regression
for(i in 1:5){
  
  training.i <- folds[[i]]
  testing.i <- setdiff(1:length(gold.r$PercChangeForc), training.i)
  gold.test <- gold.r[testing.i,]
  gold.train <- gold.r[training.i,]

  # Model
  lin.reg <- lm(PercChangeForc~., data = gold.train)
  
  # Model pred
  lin.reg.pred <- predict(lin.reg, newdata = gold.test)
  
  # Store MSE vals
  lin.reg.mse[i] <- mean((lin.reg.pred-gold.test$PercChangeForc)^2)
  
}

lin.reg.mse
mean(lin.reg.mse)

```

### Ridge & Lasso (Shanie)

### Principal Components Regression (Arif)

### Random Forest + Bagging (Jon)
```{r}
library(gbm)
set.seed(123)

```

### Boosting (Jon)
```{r}
set.seed(123)
library(caret)
library(gbm)

# # Train Function with Grid of Parameters
# control <- trainControl(method = "cv", number = 5)
# 
# # Create Parameter Grid
# parameters.r <- expand.grid(n.trees = c(100,200,500,1000,1500),
#                           interaction.depth = c(5,10,15,20,50,100),
#                           shrinkage = c(0,0.001,0.005,0.01,0.03,0.05),
#                           n.minobsinnode = c(5))
# 
# # Fit A Boosting (GBM) Model 
# features.r <- gold.r[, !colnames(gold.r) %in% "PercChangeForc"]
# 
# trained.gbm.fit.r <- train(x = features.r, y = gold.r$PercChangeForc, 
#                            method = "gbm", trControl = control, 
#                            tuneGrid = parameters.r)
# 
# trained.gbm.fit.r$bestTune

# Output:

# n.trees = 100	
# 
# interaction.depth = 5
# 
# shrinkage = 0.01
# 
# n.minobsinnode = 5




# Run Boosted Model with optimal Parameters to calculate CV MSE:

boost.mse <- vector()

for (i in 1:5){

  training.i <- folds[[i]]
  testing.i <- setdiff(1:length(gold.r$PercChangeForc), training.i)
  gold.test <- gold.r[testing.i,]
  gold.train <- gold.r[training.i,]
  
  gold.r.boost <- gbm(PercChangeForc ~ ., data = gold.train, distribution = "gaussian", 
                   n.trees = 100, interaction.depth = 5,
                   shrinkage = 0.01)

  # Made preds
  yhat <- predict(gold.r.boost, newdata = gold.test, n.trees = 100)

  # MSE calc
  boost.mse[i] <- mean((yhat - gold.test$PercChangeForc)^2)
}

boost.mse
mean(boost.mse)

```


### Neural Network
```{r}

```


--------------------------------------------------------------------------------

# Classificatio Problem

### LDA (Shanie)
```{r}

```

### QDA (Shanie)
```{r}

```

### KNN (Arif)
```{r}

```

### Random Forest (Arif)
```{r}

```

### Bagging (Arif)
```{r}

```

### Boosting (Arif)
```{r}

```


### Support Vector Machine (Arif)
```{r}

```



### Neural Network
```{r}

```









