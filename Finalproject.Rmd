---
title: "Final Project"
author: "Shanie Talor, Arif Abd Aziz, Jon Girolamo"
date: '2023-06-02'
output: pdf_document
---

# Introduction

(We don't have to polish this RMD in terms of labels or descriptions. Final presentation will be in powerpoint. We'll still submit this as the source code though so we should still be commenting what we're doing with code) Jon

# Load Data
```{r}
# Read csv
gold <- read.csv("gold.csv", sep = ",", header = TRUE)

# Read CSV with one extra parameter
sentiment <- read.csv("sentiment.csv", sep = ",", header = TRUE)

# Clean Data for Rows Missing
sentiment <- sentiment[-c(1:9, 543, 544), ] 

# Add sentiment parameter to main data set
gold$sentiment <- sentiment$sentiment

head(gold)

# Omit all rows that have NA Values: 6 Rows: 05/31/1979 to 03/31/2023
gold <- na.omit(gold)

# Add Binary Classifier Variable
gold$Binary.PercChange <- ifelse(gold$PercChangeForc > 0.1, 1, 0)

# Check if Features Are Numeric (Returns False, So Non-Numeric Variables)
all(sapply(gold, is.numeric))

# Final Check of Head
head(gold)
```

We converted our percent change forecast variable into a binary classifier. The hurdle rate for our binary classifier ("Binary.PercChange") is set at 0.1%, so that edge cases that are very close to 0% (0%:0.0999%) are classified as non-investment opportunities. Percent changes in gold (month over month) greater than 0.1% will be considered "buy" opportunities, with a classifier value of 1. 

# Initial Analysis of Features 

In order to better understand our features, we'll first look at the histograms of our features as well as the scatterplots between our features and our main response variables: monthly percent changes in gold for our regression problem and our binary classiifer for our classification problem. 

### Histogram for Loops
```{r}
# Set Feature Space Excluding Lagged Variables and Other Miscellenous Feature 
library(dplyr)
library(ggplot2)
library(stats)
library(reshape2)
gold_histograms <- gold %>% select(-c("Date","PercChangeLag1","PercChangeLag2",
                                "PercChangeLag3", "PercChangeLag4",
                                "PercChangeLag5","Inf.L1","Inf.L2",
                                "Inf.L3","Inf.L4","FedFundsRateL1"))
gold_histograms <- as.data.frame(lapply(gold_histograms, as.numeric))
head(gold_histograms)

# For Loop for Histograms
for (feature in names(gold_histograms)){
  hist(gold_histograms[[feature]], xlab = paste0(feature), ylab = "Frequency",
       main = paste0("Histogram of ",feature))
}

# For Loop for Box Plots
for (feature in names(gold_histograms)){
  boxplot(gold_histograms[[feature]], ylab = paste0("Value of ",feature),
       main = paste0("Boxplot of ",feature))
}
```

The boxplots and histograms show that our variables are on average normally distributed. There are outliers in some of our features, such as percent changes in monthly prices and changes in bank reserves. However, we believe these outliers are still essential in predicting the movement of gold prices. 

### Correlation Values
```{r} 
# Data Clean for Scatter Plots 
set.seed(490782)
gold_cor <- gold %>% select(-c("Date","Average.Price")) 
gold_cor <- as.data.frame(lapply(gold_cor, as.numeric))
head(gold_cor)

# For Loop for Scatter Plots 
for (feature in names(gold_cor)[!names(gold_cor) %in% c("Binary.PercChange")]){
  plot(gold_cor[[feature]], gold_cor$PercChangeForc, type = "p",
       xlab = paste(feature), ylab = "Monthly Forecasted Change in Gold Prices",
       main = paste0("Monthly Changes in Gold Prices Against ",feature)) 
  line <- lm(gold_cor$PercChangeForc ~ gold_cor[[feature]])
  abline(line, col = "blue")
}

# Heat Map
cor_matrix <- round(cor(gold_cor),2)
melted_cor_gold <- melt(cor_matrix)
ggplot(data = melted_cor_gold, aes(x = Var1, y = Var2, fill = value))+
  geom_tile()+theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Our scatterplots and heat maps show that there are some pockets of correlation between features in our data set. Looking at our main response variables (PercChangeForc & Binary.PercChange), there are no visibly strong correlations between them and our various features. However, we believe a combination and enhancement of these features through the various algorithms we'll be looking at will allow us to best predict both response variables - in a regression and classification setting. 

# Initial Data Cleaning
```{r}
# Gold Data for Regression Problem:
head(gold)
gold.r <- gold %>% select(-c("Date","Average.Price","Binary.PercChange","Oil",
                             "PPIJewelry"))

# Gold Data for Classification Problem:
gold.c <- gold %>% select(-c("Date","Average.Price","PercChangeForc","Oil",
                             "PPIJewelry")) 

# Convert Data to Numeric
gold.r <- as.data.frame(lapply(gold.r, as.numeric)) 
gold.c <- as.data.frame(lapply(gold.c, as.numeric))

# Data Check & Numeric Check
all(sapply(gold.r, is.numeric))
all(sapply(gold.c, is.numeric))
head(gold.r)
head(gold.c)
```

# Feature Selection: Boruta Algorithm
```{r, echo = FALSE}
# Run Boruta algorithm on clean data
library(Boruta)
library(randomForest)

# Regression Problem
boruta.gold <- Boruta(PercChangeForc~., data = gold.r, doTrace = 2, 
                       randomForest = TRUE) 

# Classification Problem
boruta.c.features <- gold.c[,!colnames(gold.c) %in% "Binary.PercChange"] # Features only
boruta.gold.c <- Boruta(boruta.c.features, as.factor(gold.c$Binary.PercChange))
```

### Plot Boruta Algorithm
```{r, echo = FALSE}
# Plots
plot(boruta.gold, main = "Boruta Algorithm for Gold Pricing Regression Problem",
     las = 3)
plot(boruta.gold.c, main = "Boruta Algorithm for Gold Pricing Classification Problem",
     las = 3)

# Boruta Selections
boruta.gold$finalDecision # Regression
boruta.gold.c$finalDecision # Classification
```

# Create Five Fold Cross Validation
```{r}
library(caret)
set.seed(123)
folds <- createFolds(gold.r$PercChangeForc, k = 5, returnTrain = TRUE)
```

# Regression

### OLS (Jon)
```{r}
set.seed(123)
# Run linear regression
lin.reg <- lm(PercChangeForc~., data = gold.r)

summary(lin.reg)

# Create empty vector
lin.reg.mse <- vector()

# CV loop for linear regression
for(i in 1:5){
  
  training.i <- folds[[i]]
  testing.i <- setdiff(1:length(gold.r$PercChangeForc), training.i)
  gold.test <- gold.r[testing.i,]
  gold.train <- gold.r[training.i,]

  # Model
  lin.reg <- lm(PercChangeForc~., data = gold.train)
  
  # Model pred
  lin.reg.pred <- predict(lin.reg, newdata = gold.test)
  
  # Store MSE vals
  lin.reg.mse[i] <- mean((lin.reg.pred-gold.test$PercChangeForc)^2)
  
}

lin.reg.mse
mean(lin.reg.mse)

```

### Ridge & Lasso (Shanie)
```{r}

```


### Principal Components Regression (Arif)

For principal components regression, we'll optimize for the number of components in our regression by minimizing mean squared errors in predicting forecast percent change. We'll use a validation plot to see what number of components minimizes errors - in this case MSEP scores. 

```{r}
# Check Validation Plots for Principal Components Regression
library(pls)
for (i in 1:length(folds)){ 
  # Set Folds
  gold.r.train <- gold.r[folds[[i]],]
  gold.r.test <- gold.r[-folds[[i]],]
  pcr.mdl.r <- pcr(PercChangeForc ~., data = gold.r.train, scale = TRUE, 
                   validation = "CV")
  validationplot(pcr.mdl.r, val.type = "MSEP", main = paste0("Percent Change in Gold Prices: MSE Performance - Fold ",i), xlab = "Number of Components")
} 

# Set Up Data Frame for Receiving Error Vectors
components.r <- 1:(length(gold.r)-1)
pcr.err.df <- data.frame(components.r)

# Fit PCR Model on Five Fold Cross Validation
for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.r.train <- gold.r[train_index,]
  gold.r.test <- gold.r[-train_index,]   
  
  # Set Error Vector to Capture MSE Scores
  mse.pcr.vector <- vector()
  
  # For Loop on Multiple Components
  for (j in 1:(length(gold.r)-1)){
    
  # Fit Regression Model
  pcr.mdl <- pcr(PercChangeForc ~., data = gold.r.train, scale = TRUE)
  
  # Predict Using PCR
  pcr.predict.r <- predict(pcr.mdl, gold.r.test, ncomp = j)
  
  # MSE Calculation & Input
  mse.pcr.vector[j] <- mean((gold.r.test$PercChangeForc-pcr.predict.r)^2)
  }
  
  # Input Vector of MSE Values for Each Fold into Data Frame
  pcr.err.df <- cbind(pcr.err.df, mse.pcr.vector)
}

# Clean Data Frame
pcr.err.df$Means <- rowMeans(pcr.err.df[,-1])
colnames(pcr.err.df) <- c("Components","Fold 1","Fold 2","Fold 3", "Fold 4",
                          "Fold 5", "MSE Means")
print(pcr.err.df) 

# Locate Minimium MSE in Components
min.pcr.error.rn <- which(pcr.err.df$`MSE Means` == min(pcr.err.df$`MSE Means`))
print(pcr.err.df[min.pcr.error.rn,]) 

# Additional Plots for Presentation: Along with MSE Plots
ggplot(data = pcr.err.df, aes(x = Components, y = `MSE Means`))+
  geom_line(color = "navy")+labs(x = "Number of Components", y = "Test MSE",
       title = "Five Fold Cross Validation: Components Performance")
```

After running five fold cross validation, we find that setting 21 components in our principal components regression optimally minimizes mean squared error. From running cross validation, our model returns an average mean squared error of **20.97**. 

### Random Forest + Bagging (Jon)
```{r}
library(gbm)
set.seed(123)

mtry.vals <- seq(1, 22, by = 1)

bag.mse <- vector()
mod.df.r <- data.frame(mtry.vals)

for (i in 1:5){
  
  training.i <- folds[[i]]
  testing.i <- setdiff(1:length(gold.r$PercChangeForc), training.i)
  gold.test <- gold.r[testing.i,]
  gold.train <- gold.r[training.i,]
  
  
  for (j in seq_along(mtry.vals)){

  mod <- randomForest(PercChangeForc~., data = gold.train, mtry = mtry.vals[j], importance = TRUE)

  mod.pred <- predict(mod, newdata = gold.test)

  bag.mse[j] <- mean((mod.pred-gold.test$PercChangeForc)^2)
  
  }
  # Append our MSE Vector into Our Data Frame (Memory)
  mod.df.r <- cbind(mod.df.r, bag.mse)
  
}
mod.df.r

mod.df.r$mean_mse <- rowMeans(mod.df.r[, -1])

# Optimal bagging mtry
opt.mtry <- mod.df.r[which.min(mod.df.r$mean_mse), ]
opt.mtry



# Optimal Random Forest MSE: mtry = 1, MSE = 21.21

# Bagging MSE: MSE = 25.70529

```

### Boosting (Jon)
```{r}
set.seed(123)
library(caret)
library(gbm)

# # Train Function with Grid of Parameters
# control <- trainControl(method = "cv", number = 5)
# 
# # Create Parameter Grid
# parameters.r <- expand.grid(n.trees = c(100,200,500,1000,1500),
#                           interaction.depth = c(5,10,15,20,50,100),
#                           shrinkage = c(0,0.001,0.005,0.01,0.03,0.05),
#                           n.minobsinnode = c(5))
# 
# # Fit A Boosting (GBM) Model 
# features.r <- gold.r[, !colnames(gold.r) %in% "PercChangeForc"]
# 
# trained.gbm.fit.r <- train(x = features.r, y = gold.r$PercChangeForc, 
#                            method = "gbm", trControl = control, 
#                            tuneGrid = parameters.r)
# 
# trained.gbm.fit.r$bestTune

# Output:

# n.trees = 100	
# 
# interaction.depth = 5
# 
# shrinkage = 0.01
# 
# n.minobsinnode = 5




# Run Boosted Model with optimal Parameters to calculate CV MSE:

boost.mse <- vector()

for (i in 1:5){

  training.i <- folds[[i]]
  testing.i <- setdiff(1:length(gold.r$PercChangeForc), training.i)
  gold.test <- gold.r[testing.i,]
  gold.train <- gold.r[training.i,]
  
  gold.r.boost <- gbm(PercChangeForc ~ ., data = gold.train, distribution = "gaussian", 
                   n.trees = 100, interaction.depth = 5,
                   shrinkage = 0.01)

  # Made preds
  yhat <- predict(gold.r.boost, newdata = gold.test, n.trees = 100)

  # MSE calc
  boost.mse[i] <- mean((yhat - gold.test$PercChangeForc)^2)
}

boost.mse
mean(boost.mse)

```


### Neural Network
```{r}

```


--------------------------------------------------------------------------------

# Classification Problem


### Some preliminary boundary visuals
```{r}
library(ggplot2)
# install.packages("gridExtra")
library(gridExtra)

# Classification plots of important variables according to Baruta

plot1 <- ggplot(gold.c, aes(x = PercChangeLag1, y = FedFundsRate, color = factor(Binary.PercChange))) +
  geom_point() +
  labs(color = "Binary.PercChange") +
  scale_color_manual(values = c("navy", "salmon")) +
  theme_linedraw() +
  theme(legend.position = "none")

plot2 <- ggplot(gold.c, aes(x = UM.Infl.Exp, y = Res.Change.Exc.Gold, color = factor(Binary.PercChange))) +
  geom_point() +
  labs(color = "Binary.PercChange") +
  scale_color_manual(values = c("navy", "salmon")) +
  theme_linedraw() +
  theme(legend.position = "none")

plot3 <- ggplot(gold.c, aes(x = X2MA, y = X3MA, color = factor(Binary.PercChange))) +
  geom_point() +
  labs(color = "Binary.PercChange") +
  scale_color_manual(values = c("navy", "salmon")) +
  theme_linedraw() +
  theme(legend.position = "none")

plot4 <- ggplot(gold.c, aes(x = NFCI, y = Indus.Prod.Ind, color = factor(Binary.PercChange))) +
  geom_point() +
  labs(color = "Binary.PercChange") +
  scale_color_manual(values = c("navy", "salmon")) +
  theme_linedraw() +
  theme(legend.position = "none")

grid.arrange(plot1, plot2, plot3, plot4, nrow = 2, ncol = 2)

```

### LDA (Shanie)
```{r}

```

### QDA (Shanie)
```{r}

```

### KNN: Using Optimal Train (Arif)
```{r}
# Optimize Model for Best K Parameters Using Train Function 
library(class)
set.seed(75849)
ctrl <- trainControl(method = "cv", number = 5) # Five Fold CV

# Parameter Grid Search
k_values <- expand.grid(k = 1:100)

# Train the Model on Grid Set Above - Parameter Grid Search
knn_optimize <- train(as.factor(Binary.PercChange) ~., data = gold.c, 
                      method = "knn", trControl = ctrl, tuneGrid = k_values)
knn_optimize$bestTune

# Optimize Models (More Manually)
knn_vector_value <- seq(3,51,2)
knn_perf_df <- data.frame(knn_vector_value)

for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.c.train <- gold.c[train_index,]
  gold.c.test <- gold.c[-train_index,] 
  
  # Set Inputs for KNN Function With Each Fold
  train_features <- gold.c.train[,-length(gold.c.train)]
  test_features <- gold.c.test[,-length(gold.c.train)]
  train_class <- gold.c.train$Binary.PercChange
  
  # Set Up Vector To Input Error Scores
  knn_error <- vector() # This vector will be wiped out with each fold run
  
  # Set Up Inner For Loop for KNN Factors, 3-51 (That Would be 50 Parameters) 
  for (j in seq(3,51,2)){
    # Run Model
    knn.mdl <- knn(train_features, test_features, train_class, k = j)
    
    # Get Accuracy Score
    knn.accuracy <- mean(knn.mdl == gold.c.test$Binary.PercChange)
    
    # Get Error Score & Input
    knn.error <- 1-knn.accuracy
    knn_error <- append(knn_error, knn.error) 
  }
  # Bind to Data Frame (Creating Five Columns for Five Folds)
  knn_perf_df <- cbind(knn_perf_df, knn_error)
}

# Clean Data Frame
knn_perf_df$KNNmeans <- rowMeans(knn_perf_df[,-1])
colnames(knn_perf_df) <- c("K Parameter", "Fold 1", "Fold 2", "Fold 3", "Fold 4",
                           "Fold 5", "Error Means") 
print(knn_perf_df) 

# Find Minimum Error for K Parameter
knn_min_rn <- which.min(knn_perf_df$`Error Means`)
knn_perf_df[knn_min_rn,] 

### Additional Plots for Presentation ###

# Five Fold CV Error Score Against Number of K Nearest Neighbors 
knn.point <- data.frame(`K Parameter` = 13, `Error Means` = 0.4133)

ggplot(knn_perf_df, aes(x = `K Parameter`, y = `Error Means`))+
         geom_line(color = "navy")+
         labs(x = "Number of K Nearest Neighbors", 
              y = "Misclassification Error Score",
              title = "KNN Five Fold CV Performance")+
  geom_text(data = knn.point, 
            aes(x = K.Parameter, y = Error.Means, label = Error.Means),
             vjust = 1.3, color = "navy", size = 3)

# Create a Scatter Plot for Fifth Fold Test Points
plot(gold.c.test$PercChangeLag1, gold.c.test$FedFundsRate, 
     col = ifelse(gold.c.test$Binary.PercChange == 1, "salmon","navy"), 
     pch = 1, xlab = "Lagged MoM Percentage Change in Gold Prices", 
     ylab = "Fed Funds Rate", main = "Gold Price Binary Classifier")
legend("bottomright", legend = c("1 as > 0.1%", "0 as < 0.1%"),
       pch = c(1,1), col = c("salmon", "navy"), cex = 0.7)

# Create Scatter Plot with Fifth Fold Test Points & KNN Predictions
plot(gold.c.test$PercChangeLag1, gold.c.test$FedFundsRate, 
     col = ifelse(gold.c.test$Binary.PercChange == 1, "salmon","navy"), 
     pch = 1, xlab = "Lagged MoM Percentage Change in Gold Prices", 
     ylab = "Fed Funds Rate", main = "Gold Price Binary Classifier with KNN Predictions")
points(gold.c.test$PercChangeLag1, gold.c.test$FedFundsRate,
       col = ifelse(knn.mdl == 1, "salmon","navy"), pch = 4) 
legend("bottomright", legend = c("Pred. 1 as > 0.1%", "Pred. 0 as < 0.1%"), 
       pch = c(4,4), col = c("salmon", "navy"), cex = 0.7)
```

Both our train hyper-grid search function and manual grid search show that the optimal number for K parameters (the number of neighboring observations) is 13. In other words, 13 observations closest to our observation of interest will be checked to see whether they identify with class 1 (above 0.1% gold price growth) or class 0 (below 0.1% gold price growth). The majority of these 13 classes will be assigned to the observation of interest. Using this algorithm, we get a mis-classification error (on average) of **41.33%**. This is below 50% and is considered better than random chance. Our manual fit is also similar to running an optimized KNN model on five fold cross validation, so we don't have to run that again. The mis-classification error of 41.33%, thus, is a comparable number to other approaches (as we run five fold cross validation for them). 

### Random Forest & Bagging (Arif)
```{r}
# Optimize Random Forest Model for Parameters Using Train Function
library(tree)
set.seed(75849)
ctrl <- trainControl(method = "cv", number = 5) 

# Set Grid of Parameters for Random Forests
rf.c.grid <- expand.grid(mtry = 1:22, ntree = c(100,200,300,400,500,1000,1500))
nrow(rf.c.grid)

# Set Up Data Frame to Input Values from Manual Grid Search
rf.perf.df <- data.frame(rf.c.grid[,1]) # First Column is mtry
rf.perf.df <- cbind(rf.perf.df, rf.c.grid[,2]) # Second Column is number of trees

# Manual For Loop Grid Search Since Train Function Refuses to Work
for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.c.train <- gold.c[train_index,]
  gold.c.test <- gold.c[-train_index,] 
  
  # Set Up Empty Vector to Input Misclassificaiton Errors for Each Fold
  rf_error_vec <- vector()
  
  # Now Set Up Grid Search For Loop Using rf.perf.df Parameter Grid
  for (j in 1:nrow(rf.perf.df)){ # Will Havet to Change Just Testing
    # Fit the Model, Mtry is the first column of grid, Ntrees is second column
    rf.model <- randomForest(as.factor(Binary.PercChange) ~.,
                             data = gold.c.train, mtry = rf.perf.df[j,1],
                             ntree = rf.perf.df[j,2])
    
    # Predict on Testing Set, Response is a Class Variable
    rf.predictions <- predict(rf.model, newdata = gold.c.test, type = "class")
    
    # Calculate Accuracy Score
    rf.accuracy <- mean(rf.predictions == gold.c.test$Binary.PercChange)
    
    # Calculate Error Scores and Input into Vector
    rf.error <- 1-rf.accuracy
    rf_error_vec[j] <- rf.error
  }
  
  # Put Error Vector into RF Data Frame to Measure Error Scores
  rf.perf.df <- cbind(rf.perf.df, rf_error_vec)
  
  # Run This Back Up Again for Five Folds, Data Frame Has Five Columns
}

# Clean Up Data Frame
rf.perf.df$Means <- rowMeans(rf.perf.df[,-c(1,2)]) 
colnames(rf.perf.df) <- c("Mtry Parameters", "Number of Trees", "Fold 1", "Fold 2",
                          "Fold 3", "Fold 4", "Fold 5", "FF CV Errors")
print(rf.perf.df) 

# Find Bagging Misclassification Errors
rf.perf.df[rf.perf.df$`Mtry Parameters` == 22,]

# Find Minimum Misclassification Error
rf_min_rn <- which.min(rf.perf.df$`FF CV Errors`)
rf.perf.df[rf_min_rn,] 

### Additional Plots for Presentation ### 

# Variable Importance
rf.mdl.pres.try <- randomForest(as.factor(Binary.PercChange) ~.,
                             data = gold.c, mtry = 18,
                             ntree = 200, importance = TRUE) 
varImpPlot(rf.mdl.pres, cex = 0.55, main = "Random Forest Variable Importance") 

# Grid Plot of MSE Performance through Five Fold
ggplot(rf.perf.df, aes(x = `Mtry Parameters`, y = `Number of Trees`,
                       size = `FF CV Errors`))+geom_point(col = "navy")+
  labs(x = "Number of Variables Sampled (Mtry)", 
       title = "Classification Random Forest Performance")

```

Since our train control function was not working, we decided to run a manual parameter grid search for number of trees (ntrees) and number of features to consider for each tree (mtry). This like the manual knn parameter search is equivalent to testing the random forest model's performance on our five cross validation folds. Thus, the results from this grid search are comparable to other classificaiton models we ran. After running random forests and bagging algorithms on our data set, we find that a random forest with parameters (mtry = 18 and number of trees = 200) is the most optimal in minimizing misclassification errors. It returned a misclassification error of **42.28%** which is slightly higher than our KNN model but still less than 50%. This shows that our model performs better than random chance when it comes to classifying the direction of gold prices month over month, given our 0.1% hurdle rate. Looking at our bagging algorithms (mtry = 22, where we use all predictors in our feature space), their misclassification errors, on average, fell in the range **44.76%-47.42*%**. The bagging algorithms were not optimal in our classification setting. 

### Boosting (Arif)
```{r}
# Conduct Parameter Search for Gradient Boosting Model Using Train Function
set.seed(75849)
ctrl <- trainControl(method = "cv", number = 5) # Five Fold CV

# Create Parameter Grid
parameters.gbm.c <- expand.grid(n.trees = c(100,200,500,1000,1500),
                                interaction.depth = c(5,10,15,20,50,100), 
                                shrinkage = c(0.001,0.01,0.05,0.075,0.1),
                                n.minobsinnode = c(5,10,15,20))

# Fit A Boosting GBM Model for Classification
features.c <- gold.c[, !colnames(gold.c) %in% "Binary.PercChange"]

# trained.gbm.fit.c <- train(x = features.c, y = as.factor(gold.c$Binary.PercChange),
#                            method = "gbm", trControl = ctrl, 
#                            tuneGrid = parameters.gbm.c)

# trained.gbm.fit.c$bestTune

### Train Function Results ###
# Number of Trees: 200
# Interaction Depth: 10
# Shrinkage: 0.075
# Minimum Number of Observations: 15
```

After running a parameter search on our data using the train function, the best tuning parameters for our classification gradient boosting model are 200 for the number of trees, 10 for the interaction depth, 0.075 for the shrinkage parameter, and 15 for the minimum number of observations in a node. We'll use these parameters to calculate the five fold cross validation misclassification error. This will allow us to compare model performance. 

```{r}
# Set Misclassification Error Vector
gbm.c.error <- vector() 

# Five Fold Cross Validation with Best Parameters
for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.c.train <- gold.c[train_index,]
  gold.c.test <- gold.c[-train_index,] 
  
  # Fit the Model with Optimal Parameters Listed Above
  gbm.mdl.c <- gbm(Binary.PercChange ~., data = gold.c.train,
                   distribution = "bernoulli", shrinkage = 0.075,
                   n.trees = 200, interaction.depth = 10, 
                   n.minobsinnode = 15)
  
  # Predict the Training Model on Testing Data
  gbm.predict <- predict(gbm.mdl.c, newdata = gold.c.test, type = "response")
  
  # Convert Response Prediction to Classifier
  gbm.class <- ifelse(gbm.predict > 0.5, 1,0)
  
  # Accuracy Scores
  accuracy <- mean(gbm.class == gold.c.test$Binary.PercChange)
  
  # Error Scores & Input
  err <- 1-accuracy
  gbm.c.error <- append(gbm.c.error,err)
}

# Print Error Vector & Retrieve Five Fold Cross Validation Mean
print(gbm.c.error)
mean(gbm.c.error)
```

After running five fold cross validation on our classification gradient boosting model, we received a mean misclassification error of **43.06%**. This is better than our classification random forest model, but it does not perform as well as our KNN model. Regardless, our misclassification error is below 50% and is better than random chance. 

### Support Vector Machine (Arif)

For support vector machines, there are two main subsets of models that we'd like to explore in our classification task. The first is running support vector machines with polynomial kernels, and the second is running support vector machines with radial kernels. When running support vector machines with polynomial kernels, we'll vary for cost and degree - the latter represents the maximum polynomial transformation of our predictors. For support vector machines with radial kernels, we'll vary for cost and gamma. We'll use the tune function to run a parameter grid search for cost and gamma/degree. If the tune function does not work, we'll run a for loop to run the grid search. In order to compare our support vector machine results to other classification models, we'll run five fold cross-validation. 

```{r}
### Polynomial Kernel: Tune Function ###
library(e1071)

# Create Polynomial Grid: First Column is Cost, Second Column is Degree
svm.param.c.poly <- expand.grid(cost = c(0.1,0.5,1,2,5,8,10,100),
                                degree = c(2,3,4,5))

# Create Data Frame to Store Misclassification Errors
svm.poly.df <- data.frame(svm.param.c.poly)

# For Loop Grid Search Using Five Fold Cross Validation
for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.c.train <- gold.c[train_index,]
  gold.c.test <- gold.c[-train_index,] 
  
  # Set Error Vector to Input Misclassification Errors for Each Fold
  svm.error.vec <- vector()
  
  # Now Begin Fitting Model with Each Fold, Varying Parameters
  for (j in 1:nrow(svm.poly.df)){
    
    # Fit Model wtih Varying Parameters: First Column = Cost, Second = Degree
    svm.mdl <- svm(as.factor(Binary.PercChange) ~., data = gold.c.train,
                   cost = svm.poly.df[j,1], degree = svm.poly.df[j,2])
    
    # Predictions of Class (Binary Classifier)
    svm.predictions <- predict(svm.mdl, newdata = gold.c.test)
    
    # Accuracy Score
    svm.accuracy <- mean(svm.predictions == gold.c.test$Binary.PercChange)
    
    # Error Score & Input Into Vector
    svm.error <- 1-svm.accuracy
    svm.error.vec[j] <- svm.error
    
    # Input Error into Data Frame in Outermost (Fold) Loop
  }
  
  # Input Error Vector into Data Frame
  svm.poly.df <- cbind(svm.poly.df, svm.error.vec)
  
  # Error Vector Gets Washed Out At Start of New Loop
}

# Clean Data Frame
svm.poly.df$Means <- rowMeans(svm.poly.df[,-c(1,2)])
colnames(svm.poly.df) <- c("Cost Parameter", "Degree Parameter","Fold 1",
                           "Fold 2", "Fold 3", "Fold 4", "Fold 5",
                           "Average Error")
print(svm.poly.df) 

# Find Minimized Misclassification Error and Optimal Parameters for Polynomial Kernel
svm.poly.min <- which(svm.poly.df$`Average Error` == min(svm.poly.df$`Average Error`))
print(svm.poly.df[svm.poly.min,]) 

### Additional Plots for Presentation ###

# Grid Search Plot
ggplot(svm.poly.df, aes(x = `Cost Parameter`, y = `Degree Parameter`,
                       size = svm.poly.df$`Average Error`))+
  geom_point(col = "navy")+labs(x = "Number of Variables Sampled (Mtry)", 
       title = "Classification Random Forest Performance")

print(svm.poly.df)
```

We performed a grid search for the most optimal cost and degree parameters within polynomial kernels and found that the most optimal parameters are a cost value of 100 and a degree value of 2-5. The cost parameter of 100 seemed to be very good at minimizing misclassification error, regardless of polynomial degree. After running five fold cross validation with our grid search, we find that a cost parameter of 100 returned a misclassification error of **42.48%**. The support vector machine model with polynomial kernel outperforms our tree models (gradient boosting, random forests, and bagging) but still underperforms our KNN model. 

```{r}
### Radial Kernel: Grid Search ###

# Create Radial Grid: First Column is Cost, Second Column is Degree
svm.param.c.radial <- expand.grid(cost = c(0.1,0.5,1,2,5,8,10,100),
                                gamma = c(0.1,0.5,1,2,5,8,10))

# Create Data Frame to Store Misclassification Errors
svm.radial.df <- data.frame(svm.param.c.radial)

# For Loop Grid Search Using Five Fold Cross Validation
for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.c.train <- gold.c[train_index,]
  gold.c.test <- gold.c[-train_index,] 
  
  # Set Error Vector to Input Misclassification Errors for Each Fold
  svm.error.vec <- vector()
  
  # Now Begin Fitting Model with Each Fold, Varying Parameters
  for (j in 1:nrow(svm.radial.df)){
    
    # Fit Model wtih Varying Parameters: First Column = Cost, Second = Degree
    svm.mdl <- svm(as.factor(Binary.PercChange) ~., data = gold.c.train, 
                   kernel = "radial",
                   cost = svm.radial.df[j,1], gamma = svm.radial.df[j,2])
    
    # Predictions of Class (Binary Classifier)
    svm.predictions <- predict(svm.mdl, newdata = gold.c.test)
    
    # Accuracy Score
    svm.accuracy <- mean(svm.predictions == gold.c.test$Binary.PercChange)
    
    # Error Score & Input Into Vector
    svm.error <- 1-svm.accuracy
    svm.error.vec[j] <- svm.error
    
    # Input Error into Data Frame in Outermost (Fold) Loop
  }
  
  # Input Error Vector into Data Frame
  svm.radial.df <- cbind(svm.radial.df, svm.error.vec)
  
  # Error Vector Gets Washed Out At Start of New Loop
}

# Clean Data Frame
svm.radial.df$Means <- rowMeans(svm.radial.df[,-c(1,2)])
colnames(svm.radial.df) <- c("Cost Parameter", "Gamma Parameter","Fold 1",
                           "Fold 2", "Fold 3", "Fold 4", "Fold 5",
                           "Average Error")
print(svm.radial.df) 

# Find Minimized Misclassification Error and Optimal Parameters for Polynomial Kernel
svm.radial.min <- which(svm.radial.df$`Average Error` == min(svm.radial.df$`Average Error`))
print(svm.radial.df[svm.radial.min,]) 

# SVM Plots: Maybe Don't Use This
svm.radial.full <- svm(as.factor(Binary.PercChange) ~., data = gold.c, 
                       kernel = "radial", cost = 1, gamma = 0.1)
plot(svm.radial.full, gold.c, PercChangeLag1 ~ FedFundsRate, 
     xlab = "Fed Funds Rate (Monthly Average)", ylab = "Gold Price Lagged Percent Change")
```

After performing a grid search on cost and gamma parameters for our support vector machine with radial kernels, we find that the most optimal parameter values are a cost parameter of 1 and a gamma parameter of 0.1. These parameter values return a misclassification error of **41.90%**. This edges our polynomial kernels, but it still slightly underperforms our KNN model - though it is the closest among the classification models to the KNN model.  

### ROC Graph for Classification Models
```{r}
# Data Frame for Predicted Probability Values & Actual Values
empty_vector <- 1:nrow(gold.c)
roc.df <- data.frame(empty_vector)

# Vectors for Predicted Models/Actual observations: Run This Before Each Fold
gold.c.classes <- vector()
lda.pred.vector <- vector()
qda.pred.vector <- vector()
knn.pred.vector <- vector()
rf.pred.vector <- vector()
boost.pred.vector <- vector()
svm.pred.vector <- vector()

# Fitting the Models with Five Folds
for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.c.train <- gold.c[train_index,]
  gold.c.test <- gold.c[-train_index,]  
  
  # Append Actual Observations to a Vector
  gold.c.classes <- append(gold.c.classes, gold.c.test$Binary.PercChange)
  
  # Set Inputs for KNN Function With Each Fold
  train_features <- gold.c.train[,-length(gold.c.train)]
  test_features <- gold.c.test[,-length(gold.c.train)]
  train_class <- gold.c.train$Binary.PercChange
  
  # Fit Various Models to Folds 
  knn.mdl <- knn(train_features, test_features, train_class, k = 13, prob = TRUE)
  rf.model <- randomForest(as.factor(Binary.PercChange) ~.,
                             data = gold.c.train, mtry = 18,
                             ntree = 200)
  gbm.mdl.c <- gbm(Binary.PercChange ~., data = gold.c.train,
                   distribution = "bernoulli", shrinkage = 0.075,
                   n.trees = 200, interaction.depth = 10, 
                   n.minobsinnode = 15)
  svm.mdl <- svm(as.factor(Binary.PercChange) ~., data = gold.c.train,
                 kernel = "radial", cost = 1, gamma = 0.1, prob = TRUE)
  
  # Predict on Various Models
  knn.pred <- attr(knn.mdl, "prob")
  rf.pred <- predict(rf.model, gold.c.test, type = "prob")
  gbm.pred <- predict(gbm.mdl.c, gold.c.test, type = "response")
  svm.pred <- predict(svm.mdl, gold.c.test, prob = TRUE)
  
  # Append Predictions to Predict Vectors
  knn.pred.vector <- append(knn.pred.vector, knn.pred) 
  rf.pred.vector <- append(rf.pred.vector, rf.pred[,2])
  boost.pred.vector <- append(boost.pred.vector, gbm.pred)
  svm.pred.vector <- append(svm.pred.vector, svm.pred)
  
}
 
# ROC Curves
library(ROCR)
roc_knn <- pROC::roc(gold.c.classes, knn.pred.vector)
roc_rf <- pROC::roc(gold.c.classes, rf.pred.vector)
roc_gbm <- pROC::roc(gold.c.classes, boost.pred.vector)
roc_svm <- pROC::roc(gold.c.classes, svm.pred.vector)

# Individual Plots
plot(roc_knn, main = "ROC Curve", print.auc = TRUE, col = "navy") 
plot(roc_rf, main = "ROC Curve", print.auc = TRUE, col = "navy")
plot(roc_gbm, main = "ROC Curve", print.auc = TRUE, col = "navy")
plot(roc_svm, main = "ROC Curve", print.auc = TRUE, col = "navy")

# All Together Now
plot(roc_knn, main = "ROC Curve", col = "navy") 
lines(roc_rf, col = "magenta")
lines(roc_gbm, col = "green")
lines(roc_svm, col = "brown") 
legend("bottomright", legend = c("KNN", "RF","GBM","SVM"), 
       col = c("navy", "magenta","green","brown"), 
       lwd = 2, cex = 0.6,
       lty = c(1, 1), title = "Lines")

pROC::auc(roc_knn) 
pROC::auc(roc_rf) 
pROC::auc(roc_gbm) 
pROC::auc(roc_svm) 
```


### Neural Network
```{r}

```









