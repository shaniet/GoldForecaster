---
title: "Final Project"
author: "Shanie Talor, Arif Abd Aziz, Jon Girolamo"
date: '2023-06-02'
output: pdf_document
---

# Introduction

(We don't have to polish this RMD in terms of labels or descriptions. Final presentation will be in powerpoint. We'll still submit this as the source code though so we should still be commenting what we're doing with code) Jon

# Load Data
```{r}
# Read csv
gold <- read.csv("gold.csv", sep = ",", header = TRUE)

# Read CSV with one extra parameter
sentiment <- read.csv("sentiment.csv", sep = ",", header = TRUE)

# Clean Data for Rows Missing
sentiment <- sentiment[-c(1:9, 543, 544), ] 

# Add sentiment parameter to main data set
gold$sentiment <- sentiment$sentiment

head(gold)

# Omit all rows that have NA Values: 6 Rows: 05/31/1979 to 03/31/2023
gold <- na.omit(gold)

# Add Binary Classifier Variable
gold$Binary.PercChange <- ifelse(gold$PercChangeForc > 0.1, 1, 0)

# Check if Features Are Numeric (Returns False, So Non-Numeric Variables)
all(sapply(gold, is.numeric))

# Final Check of Head
head(gold)
```

We converted our percent change forecast variable into a binary classifier. The hurdle rate for our binary classifier ("Binary.PercChange") is set at 0.1%, so that edge cases that are very close to 0% (0%:0.0999%) are classified as non-investment opportunities. Percent changes in gold (month over month) greater than 0.1% will be considered "buy" opportunities, with a classifier value of 1. 

# Initial Analysis of Features 

In order to better understand our features, we'll first look at the histograms of our features as well as the scatterplots between our features and our main response variables: monthly percent changes in gold for our regression problem and our binary classiifer for our classification problem. 

### Histogram for Loops
```{r}
# Set Feature Space Excluding Lagged Variables and Other Miscellenous Feature 
library(dplyr)
library(ggplot2)
library(stats)
library(reshape2)
gold_histograms <- gold %>% select(-c("Date","PercChangeLag1","PercChangeLag2",
                                "PercChangeLag3", "PercChangeLag4",
                                "PercChangeLag5","Inf.L1","Inf.L2",
                                "Inf.L3","Inf.L4","FedFundsRateL1"))
gold_histograms <- as.data.frame(lapply(gold_histograms, as.numeric))
head(gold_histograms)

# For Loop for Histograms
for (feature in names(gold_histograms)){
  hist(gold_histograms[[feature]], xlab = paste0(feature), ylab = "Frequency",
       main = paste0("Histogram of ",feature))
}


main.hist <- ggplot(gold, aes(x = PercChangeForc)) +
  geom_histogram(binwidth = 2, fill = "navy", color = "lightgrey") +
  labs(x = "Gold Price % Change", y = "Frequency") +
  ggtitle("Percent Change of Gold Price") +
  coord_cartesian(xlim = c(-20, 25))+
  theme_minimal()
ggsave(file = "~/Desktop/main_hist.png", plot = main.hist, width = 10, height = 6, bg = "white")




# For Loop for Box Plots
for (feature in names(gold_histograms)){
  boxplot(gold_histograms[[feature]], ylab = paste0("Value of ",feature),
       main = paste0("Boxplot of ",feature))
}
```

The boxplots and histograms show that our variables are on average normally distributed. There are outliers in some of our features, such as percent changes in monthly prices and changes in bank reserves. However, we believe these outliers are still essential in predicting the movement of gold prices.

```{r}
# library(ggplot2)
# library(gridExtra)

hist_plots <- list()

for (feature in names(gold_histograms)) {
  # Create a temporary data frame with a single column for the current feature
  temp_df <- data.frame(x = gold_histograms[[feature]])

  hist_plot <- ggplot(temp_df, aes(x = x)) +
    geom_histogram(binwidth = 2, fill = "navy", color = "lightgrey") +
    labs(x = feature, y = "Frequency") +
    ggtitle(paste0("Histogram of ", feature)) +
    theme_minimal()
  hist_plots[[feature]] <- hist_plot
}

grid <- do.call(grid.arrange, c(hist_plots, ncol = 6))

ggsave(file = "~/Desktop/histogram_grid.png", plot = grid, width = 31.5, height = 14.4, bg = "white")

```


### Correlation Values
```{r} 
# Data Clean for Scatter Plots 
set.seed(490782)
gold_cor <- gold %>% select(-c("Date","Average.Price")) 
gold_cor <- as.data.frame(lapply(gold_cor, as.numeric))
head(gold_cor)

# For Loop for Scatter Plots 
for (feature in names(gold_cor)[!names(gold_cor) %in% c("Binary.PercChange")]){
  plot(gold_cor[[feature]], gold_cor$PercChangeForc, type = "p",
       xlab = paste(feature), ylab = "Monthly Forecasted Change in Gold Prices",
       main = paste0("Monthly Changes in Gold Prices Against ",feature)) 
  line <- lm(gold_cor$PercChangeForc ~ gold_cor[[feature]])
  abline(line, col = "blue")
}

# Heat Map
cor_matrix <- round(cor(gold_cor),2)
melted_cor_gold <- melt(cor_matrix)
ggplot(data = melted_cor_gold, aes(x = Var1, y = Var2, fill = value))+
  geom_tile()+theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Our scatterplots and heat maps show that there are some pockets of correlation between features in our data set. Looking at our main response variables (PercChangeForc & Binary.PercChange), there are no visibly strong correlations between them and our various features. However, we believe a combination and enhancement of these features through the various algorithms we'll be looking at will allow us to best predict both response variables - in a regression and classification setting. 

# Initial Data Cleaning
```{r}
# Gold Data for Regression Problem:
head(gold)
gold.r <- gold %>% select(-c("Date","Average.Price","Binary.PercChange","Oil",
                             "PPIJewelry"))

# Gold Data for Classification Problem:
gold.c <- gold %>% select(-c("Date","Average.Price","PercChangeForc","Oil",
                             "PPIJewelry")) 

# Convert Data to Numeric
gold.r <- as.data.frame(lapply(gold.r, as.numeric)) 
gold.c <- as.data.frame(lapply(gold.c, as.numeric))

# Data Check & Numeric Check
all(sapply(gold.r, is.numeric))
all(sapply(gold.c, is.numeric))
head(gold.r)
head(gold.c)
```

# Feature Selection: Boruta Algorithm
```{r, echo = FALSE}
# Run Boruta algorithm on clean data
library(Boruta)
library(randomForest)

# Regression Problem
boruta.gold <- Boruta(PercChangeForc~., data = gold.r, doTrace = 2, 
                       randomForest = TRUE) 

# Classification Problem
boruta.c.features <- gold.c[,!colnames(gold.c) %in% "Binary.PercChange"] # Features only
boruta.gold.c <- Boruta(boruta.c.features, as.factor(gold.c$Binary.PercChange))
```

### Plot Boruta Algorithm
```{r, echo = FALSE}
# Plots
plot(boruta.gold, main = "Boruta Algorithm for Gold Pricing Regression Problem",
     las = 3, cex.axis = 0.5, xlab = "")
plot(boruta.gold.c, main = "Boruta Algorithm for Gold Pricing Classification Problem",
     las = 3)

# Boruta Selections
boruta.gold$finalDecision # Regression
boruta.gold.c$finalDecision # Classification
```

# Create Five Fold Cross Validation
```{r}
library(caret)
set.seed(123)
folds <- createFolds(gold.r$PercChangeForc, k = 5, returnTrain = TRUE)
```

# Regression

### OLS 
```{r}
set.seed(123)
# Run linear regression
lin.reg <- lm(PercChangeForc~., data = gold.r)

summary(lin.reg)

# Create empty vector
lin.reg.mse <- vector()

# CV loop for linear regression
for(i in 1:5){
  
  training.i <- folds[[i]]
  testing.i <- setdiff(1:length(gold.r$PercChangeForc), training.i)
  gold.test <- gold.r[testing.i,]
  gold.train <- gold.r[training.i,]

  # Model
  lin.reg <- lm(PercChangeForc~., data = gold.train)
  
  # Model pred
  lin.reg.pred <- predict(lin.reg, newdata = gold.test)
  
  # Store MSE vals
  lin.reg.mse[i] <- mean((lin.reg.pred-gold.test$PercChangeForc)^2)
  
}

lin.reg.mse
mean(lin.reg.mse)

```

### Ridge & Lasso
```{r}
# Load required library
library(glmnet)

# Convert the data to matrix format
X <- as.matrix(gold.r[, -1]) # takes out response variable
y <- as.matrix(gold.r$PercChangeForc)

# Create a matrix of cross-validation folds

# Initialize vectors to store MSEs
ridgeMSEs <- c()
lassoMSEs <- c()

# Perform ridge and lasso regression with cross-validation
for (i in 1:length(folds)) {
  # Split the data into training and testing sets based on the current fold
  trainData <- X[folds[[i]], ] # training and testing are switched here
  trainLabels <- y[folds[[i]]]
  testData <- X[-folds[[i]], ]
  testLabels <- y[-folds[[i]]]
  
  # Perform ridge regression
  ridgeModel <- cv.glmnet(trainData, trainLabels, alpha = 0)
  ridgePredictions <- predict(ridgeModel, newx = testData, s = "lambda.min")
  ridgeMSE <- mean((testLabels - ridgePredictions)^2)
  ridgeMSEs <- c(ridgeMSEs, ridgeMSE)
  
  # Perform lasso regression
  lassoModel <- cv.glmnet(trainData, trainLabels, alpha = 1)
  lassoPredictions <- predict(lassoModel, newx = testData, s = "lambda.min")
  lassoMSE <- mean((testLabels - lassoPredictions)^2)
  lassoMSEs <- c(lassoMSEs, lassoMSE)
}

# Find the best ridge model
bestRidgeIndex <- which.min(ridgeMSEs)
bestRidgeMSE <- ridgeMSEs[bestRidgeIndex]
bestRidgeModel <- cv.glmnet(X, y, alpha = 0)
bestRidgePredictions <- predict(bestRidgeModel, newx = X, s = "lambda.min")

# Find the best lasso model
bestLassoIndex <- which.min(lassoMSEs)
bestLassoMSE <- lassoMSEs[bestLassoIndex]
bestLassoModel <- cv.glmnet(X, y, alpha = 1)
bestLassoPredictions <- predict(bestLassoModel, newx = X, s = "lambda.min")

# Print the MSE for the best ridge and lasso models
print(paste("Best Ridge Regression MSE:", bestRidgeMSE))
print(paste("Best Lasso Regression MSE:", bestLassoMSE))

plot(bestRidgeModel, xvar = "lambda", label = TRUE)
title("Ridge Regression Coefficient Profiles")

# Plot the coefficient profiles for lasso regression
plot(bestLassoModel, xvar = "lambda", label = TRUE)
title("Lasso Regression Coefficient Profiles")

```


### Principal Components Regression

For principal components regression, we'll optimize for the number of components in our regression by minimizing mean squared errors in predicting forecast percent change. We'll use a validation plot to see what number of components minimizes errors - in this case MSEP scores. 

```{r}
# Check Validation Plots for Principal Components Regression
library(pls)
for (i in 1:length(folds)){ 
  # Set Folds
  gold.r.train <- gold.r[folds[[i]],]
  gold.r.test <- gold.r[-folds[[i]],]
  pcr.mdl.r <- pcr(PercChangeForc ~., data = gold.r.train, scale = TRUE, 
                   validation = "CV")
  validationplot(pcr.mdl.r, val.type = "MSEP", main = paste0("Percent Change in Gold Prices: MSE Performance - Fold ",i), xlab = "Number of Components")
} 

# Set Up Data Frame for Receiving Error Vectors
components.r <- 1:(length(gold.r)-1)
pcr.err.df <- data.frame(components.r)

# Fit PCR Model on Five Fold Cross Validation
for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.r.train <- gold.r[train_index,]
  gold.r.test <- gold.r[-train_index,]   
  
  # Set Error Vector to Capture MSE Scores
  mse.pcr.vector <- vector()
  
  # For Loop on Multiple Components
  for (j in 1:(length(gold.r)-1)){
    
  # Fit Regression Model
  pcr.mdl <- pcr(PercChangeForc ~., data = gold.r.train, scale = TRUE)
  
  # Predict Using PCR
  pcr.predict.r <- predict(pcr.mdl, gold.r.test, ncomp = j)
  
  # MSE Calculation & Input
  mse.pcr.vector[j] <- mean((gold.r.test$PercChangeForc-pcr.predict.r)^2)
  }
  
  # Input Vector of MSE Values for Each Fold into Data Frame
  pcr.err.df <- cbind(pcr.err.df, mse.pcr.vector)
}

# Clean Data Frame
pcr.err.df$Means <- rowMeans(pcr.err.df[,-1])
colnames(pcr.err.df) <- c("Components","Fold 1","Fold 2","Fold 3", "Fold 4",
                          "Fold 5", "MSE Means")
print(pcr.err.df) 

# Locate Minimium MSE in Components
min.pcr.error.rn <- which(pcr.err.df$`MSE Means` == min(pcr.err.df$`MSE Means`))
print(pcr.err.df[min.pcr.error.rn,]) 

# Additional Plots for Presentation: Along with MSE Plots
ggplot(data = pcr.err.df, aes(x = Components, y = `MSE Means`))+
  geom_line(color = "navy")+labs(x = "Number of Components", y = "Test MSE",
       title = "Five Fold Cross Validation: Components Performance")
```

After running five fold cross validation, we find that setting 21 components in our principal components regression optimally minimizes mean squared error. From running cross validation, our model returns an average mean squared error of **20.97**. 

### Random Forest + Bagging
```{r}
library(gbm)
set.seed(123)

# mtry.vals <- seq(1, 22, by = 1)
# 
# bag.mse <- vector()
# mod.df.r <- data.frame(mtry.vals)
# 
# for (i in 1:5){
#   
#   training.i <- folds[[i]]
#   testing.i <- setdiff(1:length(gold.r$PercChangeForc), training.i)
#   gold.test <- gold.r[testing.i,]
#   gold.train <- gold.r[training.i,]
#   
#   
#   for (j in seq_along(mtry.vals)){
# 
#   mod <- randomForest(PercChangeForc~., data = gold.train, mtry = mtry.vals[j], importance = TRUE)
# 
#   mod.pred <- predict(mod, newdata = gold.test)
# 
#   bag.mse[j] <- mean((mod.pred-gold.test$PercChangeForc)^2)
#   
#   }
#   # Append our MSE Vector into Our Data Frame (Memory)
#   mod.df.r <- cbind(mod.df.r, bag.mse)
#   
# }
# mod.df.r
# 
# mod.df.r$mean_mse <- rowMeans(mod.df.r[, -1])
# 
# # Optimal bagging mtry
# opt.mtry <- mod.df.r[which.min(mod.df.r$mean_mse), ]
# opt.mtry
# 
# # plot(randomForest(PercChangeForc~., data = gold.train, mtry = 1, importance = TRUE))
# 
# # Optimal Random Forest MSE: mtry = 1, MSE = 21.21
# 
# # Bagging MSE: MSE = 25.70529


# Optimize Random Forest Model for Parameters
library(tree)
set.seed(75849)

# Set Grid of Parameters for Random Forests
rf.c.grid <- expand.grid(mtry = 1:22, ntree = c(100,200,300,400,500,1000,1500))
nrow(rf.c.grid)


# Set Up Data Frame to Input Values from Manual Grid Search
rf.perf.df <- data.frame(rf.c.grid[,1]) # First Column is mtry
rf.perf.df <- cbind(rf.perf.df, rf.c.grid[,2]) # Second Column is number of trees

# Manual For Loop Grid Search
for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.r.train <- gold.r[train_index,]
  gold.r.test <- gold.r[-train_index,] 
  
  # Set Up Empty Vector to Input MSE for Each Fold
  rf_mse_vec <- vector()
  
  # Now Set Up Grid Search For Loop Using rf.perf.df Parameter Grid
  for (j in 1:nrow(rf.perf.df)){
    rf.model <- randomForest(PercChangeForc ~.,
                             data = gold.r.train, mtry = rf.perf.df[j,1],
                             ntree = rf.perf.df[j,2])
    
    # Predict on Testing Set
    rf.predictions <- predict(rf.model, newdata = gold.r.test)
    
    # Calculate MSE
    rf.mse <- mean((rf.predictions-gold.r.test$PercChangeForc)^2)
    
    # Input into Vector
    rf_mse_vec[j] <- rf.mse
  }
  
  # Put Error Vector into RF Data Frame to Measure MSE
  rf.perf.df <- cbind(rf.perf.df, rf_mse_vec)
  
  # Run This Back Up Again for Five Folds, Data Frame Has Five Columns
}

# Clean Up Data Frame
rf.perf.df$Means <- rowMeans(rf.perf.df[,-c(1,2)]) 
colnames(rf.perf.df) <- c("Mtry Parameters", "Number of Trees", "Fold 1", "Fold 2",
                          "Fold 3", "Fold 4", "Fold 5", "MSE")
print(rf.perf.df) 

plot(rf.perf.df$`Mtry Parameters`, rf.perf.df$MSE)

# Find the line with the lowest MSE
lowest_mse_line <- which.min(rf.perf.df$MSE)

print(rf.perf.df[lowest_mse_line, ])

# OPTIMAL RANDOM FOREST PARAMETERS:
# MTRY = 3
# Number of Trees = 300
# Corresponding MSE = 21.05649


rf.perf.df[rf.perf.df$`Mtry Parameters`==22,]
# OPTIMAL BAGGING TREE COUNT:
# 500 trees had the lowest MSE of all the mtry = 22 iterations
# Corresponding MSE = 24.44521


# Grid Plot of MSE Performance through Five Fold
reg.tree.mse.plot <- ggplot(rf.perf.df, aes(x = `Mtry Parameters`, y = `Number of Trees`,
                       size = `MSE`))+geom_point(col = "navy")+
  labs(x = "Number of Variables Sampled (Mtry)", 
       title = "Regression Random Forest Performance")+
  theme_classic()


ggsave(file = "~/Desktop/reg_plot.png", plot = reg.tree.mse.plot, width = 9, height = 6, bg = "white")

```

### Boosting
```{r}
set.seed(123)
library(caret)
library(gbm)

# Train Function with Grid of Parameters
control <- trainControl(method = "cv", number = 5)

# Create Parameter Grid
parameters.r <- expand.grid(n.trees = c(100,200,500,1000,1500),
                          interaction.depth = c(5,10,15,20,50,100),
                          shrinkage = c(0,0.001,0.005,0.01,0.03,0.05),
                          n.minobsinnode = c(5))

# Fit A Boosting (GBM) Model
features.r <- gold.r[, !colnames(gold.r) %in% "PercChangeForc"]

trained.gbm.fit.r <- train(x = features.r, y = gold.r$PercChangeForc,
                           method = "gbm", trControl = control,
                           tuneGrid = parameters.r)

trained.gbm.fit.r$bestTune

# Output:

# n.trees = 100
#
# interaction.depth = 5
#
# shrinkage = 0.01
#
# n.minobsinnode = 5




# Run Boosted Model with optimal Parameters to calculate CV MSE:

boost.mse <- vector()

for (i in 1:5){

  training.i <- folds[[i]]
  testing.i <- setdiff(1:length(gold.r$PercChangeForc), training.i)
  gold.test <- gold.r[testing.i,]
  gold.train <- gold.r[training.i,]
  
  gold.r.boost <- gbm(PercChangeForc ~ ., data = gold.train, distribution = "gaussian", 
                   n.trees = 100, interaction.depth = 5,
                   shrinkage = 0.01)

  # Made preds
  yhat <- predict(gold.r.boost, newdata = gold.test, n.trees = 100)

  # MSE calc
  boost.mse[i] <- mean((yhat - gold.test$PercChangeForc)^2)
}

boost.mse
mean(boost.mse)

```


### Neural Network

Disclaimer: The neural network models ran on one of our markdown files, but can't on this one. In our final presentation, we added our findings from our neural network model. We won't run it in this document because it won't knit. 

```{r}
# library(keras)
# library(tensorflow)
# library(reticulate) 
# # Define the parameter grid
# layers_grid <- c(1, 2, 3)  # Different numbers of layers
# neurons_grid <- c(32, 64, 128)  # Different numbers of neurons
# 
# # Initialize variables to store the best configuration and MSE
# best_layers <- NULL
# best_neurons <- NULL
# best_mse <- Inf
# 
# # Perform grid search
# for (layers in layers_grid) {
#   for (neurons in neurons_grid) {
#     # Create the sequential model
#     model <- keras_model_sequential()
#     model %>%
#       layer_dense(units = neurons, activation = "relu", input_shape = ncol(gold.r) - 1)
#     for (i in seq(layers - 1)) {
#       model %>%
#         layer_dense(units = neurons, activation = "relu")
#     }
#     model %>%
#       layer_dense(units = 1)
# 
#     # Compile the model
#     model %>% compile(
#       loss = "mean_squared_error",
#       optimizer = "adam",
#       metrics = c("mse")
#     )
# 
#     # Train the model
#     history <- model %>% fit(
#       x = as.matrix(gold.r[, -ncol(gold.r)]),
#       y = as.matrix(gold.r$PercChangeForc),
#       epochs = 10,
#       batch_size = 32,
#       validation_split = 0.2
#     )
# 
#     # Calculate the MSE
#     mse <- history$metrics$val_mse[length(history$metrics$val_mse)]
# 
#     # Check if the current configuration is the best so far
#     if (mse < best_mse) {
#       best_layers <- layers
#       best_neurons <- neurons
#       best_mse <- mse
#       best_history <- history
# 
#     }
#   }
# }
# 
# # Print the best configuration and MSE
# print(paste("Best Layers:", best_layers))
# print(paste("Best Neurons:", best_neurons))
# print(paste("Best MSE:", best_mse))
# 
# # Plot the training history of the best model
# plot(best_history$metrics$loss, type = "l", col = "blue", xlab = "Epoch", ylab = "Loss",
#      main = "Training History - Best Model")
# lines(best_history$metrics$val_loss, col = "red")
# legend("topright", legend = c("Training Loss", "Validation Loss"), col = c("blue", "red"), lty = 1)

```

# Classification Problem

### Preliminary boundary visuals
```{r}
library(ggplot2)
# install.packages("gridExtra")
library(gridExtra)

# Classification plots of important variables according to Baruta

plot1 <- ggplot(gold.c, aes(x = PercChangeLag1, y = FedFundsRate, color = factor(Binary.PercChange))) +
  geom_point() +
  labs(x = "Gold Price % Change", y = "Federal Funds Rate",color = "Binary.PercChange") +
  scale_color_manual(values = c("navy", "salmon")) +
  theme_linedraw() +
  theme(legend.position = "none")

plot2 <- ggplot(gold.c, aes(x = UM.Infl.Exp, y = Res.Change.Exc.Gold, color = factor(Binary.PercChange))) +
  geom_point() +
  labs(x = "Inflation Expectation", y = "US Bank Reserves (less gold)", color = "Binary.PercChange") +
  scale_color_manual(values = c("navy", "salmon")) +
  theme_linedraw() +
  theme(legend.position = "none")

plot3 <- ggplot(gold.c, aes(x = X2MA, y = X3MA, color = factor(Binary.PercChange))) +
  geom_point() +
  labs(x = "2 Month Moving Average Gold Price % Change", y = "2 Month Moving Average Gold Price % Change", color = "Binary.PercChange") +
  scale_color_manual(values = c("navy", "salmon")) +
  theme_linedraw() +
  theme(legend.position = "none")

plot4 <- ggplot(gold.c, aes(x = NFCI, y = Indus.Prod.Ind, color = factor(Binary.PercChange))) +
  geom_point() +
  labs(x = "Market Sentiment Index", y = "Industrial Production",color = "Binary.PercChange") +
  scale_color_manual(values = c("navy", "salmon")) +
  theme_linedraw() +
  theme(legend.position = "none")

grid.plot <- grid.arrange(plot1, plot2, plot3, plot4, nrow = 2, ncol = 2)
grid.plot

ggsave(file = "~/Desktop/grid_plot.png", plot = grid.plot, width = 10, height = 6, bg = "white")

```


### LDA/QDA
```{r}
# Load required libraries
library(ROCR)
library(ggplot2)
library(MASS)

# Fit LDA model
ldaModel <- lda(as.factor(Binary.PercChange) ~ ., data = gold.c)

# Fit QDA model using selected variables
qdaFeatures <- c("PercChangeLag1", "Indus.Prod.Ind", "FedFundsRate")
qdaModel <- qda(as.factor(Binary.PercChange) ~ ., data = gold.c[, c(qdaFeatures, "Binary.PercChange")])

# Compute predicted probabilities
ldaPred <- predict(ldaModel, newdata = gold.c)$posterior[, 2]  # Use class 1 posterior probability
qdaPred <- predict(qdaModel, newdata = gold.c[, qdaFeatures])$posterior[, 2]  # Use class 1 posterior probability

# Create a prediction object for LDA
ldaPrediction <- prediction(ldaPred, gold.c$Binary.PercChange)

# Create a prediction object for QDA
qdaPrediction <- prediction(qdaPred, gold.c$Binary.PercChange)

# Create ROC curve for LDA
ldaROC <- performance(ldaPrediction, "tpr", "fpr")
ldaAUC <- performance(ldaPrediction, "auc")@y.values[[1]]

# Create ROC curve for QDA
qdaROC <- performance(qdaPrediction, "tpr", "fpr")
qdaAUC <- performance(qdaPrediction, "auc")@y.values[[1]]

# Plot ROC curves
plot(ldaROC, col = "blue", main = "ROC Curve - LDA vs QDA")
plot(qdaROC, col = "red", add = TRUE)
legend("bottomright", legend = c(paste0("LDA (AUC = ", ldaAUC, ")"), paste0("QDA (AUC = ", qdaAUC, ")")), col = c("blue", "red"), lty = 1)

# Initialize vectors to store accuracy scores
ldaAccuracies <- c()
qdaAccuracies <- c()

# Perform cross-validation
for (i in 1:length(folds)) {
  # Split the data into training and testing sets based on the current fold
  trainData <- gold.c[folds[[i]], ]
  testData <- gold.c[-folds[[i]], ]
  
  # Perform LDA
  ldaModel <- lda(as.factor(Binary.PercChange) ~ ., data = trainData)
  ldaPredictions <- predict(ldaModel, newdata = testData)$class
  
  # Calculate accuracy for LDA
  ldaAccuracy <- sum(ldaPredictions == testData$Binary.PercChange) / length(testData$Binary.PercChange)
  ldaAccuracies <- c(ldaAccuracies, ldaAccuracy)
  
  # Perform QDA using selected variables
  qdaFeatures <- c("PercChangeLag1", "Indus.Prod.Ind", "FedFundsRate")
  qdaModel <- qda(as.factor(Binary.PercChange) ~ ., data = trainData[, c(qdaFeatures, "Binary.PercChange")])
  qdaPredictions <- predict(qdaModel, newdata = testData[, qdaFeatures])$class
  
  # Calculate accuracy for QDA
  qdaAccuracy <- sum(qdaPredictions == testData$Binary.PercChange) / length(testData$Binary.PercChange)
  qdaAccuracies <- c(qdaAccuracies, qdaAccuracy)
}


# Create a data frame with accuracy scores
accuracyData <- data.frame(Model = rep(c("LDA", "QDA"), each = length(folds)),
                           Accuracy = c(ldaAccuracies, qdaAccuracies))

# Plot the violin plot
ggplot(accuracyData, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_violin() +
  xlab("Model") +
  ylab("Accuracy") +
  ggtitle("Comparison of LDA and QDA Accuracy") +
  scale_fill_manual(values = c("blue", "red"))
# Calculate mean accuracy scores
ldaMeanAccuracy <- mean(ldaAccuracies)
qdaMeanAccuracy <- mean(qdaAccuracies)

# Print the mean accuracy scores
print(paste("LDA Mean Accuracy:", ldaMeanAccuracy))
print(paste("QDA Mean Accuracy:", qdaMeanAccuracy))

```

### KNN: Using Optimal Train
```{r}
# Optimize Model for Best K Parameters Using Train Function 
library(class)
set.seed(75849)
ctrl <- trainControl(method = "cv", number = 5) # Five Fold CV

# Parameter Grid Search
k_values <- expand.grid(k = 1:100)

# Train the Model on Grid Set Above - Parameter Grid Search
knn_optimize <- train(as.factor(Binary.PercChange) ~., data = gold.c, 
                      method = "knn", trControl = ctrl, tuneGrid = k_values)
knn_optimize$bestTune

# Optimize Models (More Manually)
knn_vector_value <- seq(3,51,2)
knn_perf_df <- data.frame(knn_vector_value)

for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.c.train <- gold.c[train_index,]
  gold.c.test <- gold.c[-train_index,] 
  
  # Set Inputs for KNN Function With Each Fold
  train_features <- gold.c.train[,-length(gold.c.train)]
  test_features <- gold.c.test[,-length(gold.c.train)]
  train_class <- gold.c.train$Binary.PercChange
  
  # Set Up Vector To Input Error Scores
  knn_error <- vector() # This vector will be wiped out with each fold run
  
  # Set Up Inner For Loop for KNN Factors, 3-51 (That Would be 50 Parameters) 
  for (j in seq(3,51,2)){
    # Run Model
    knn.mdl <- knn(train_features, test_features, train_class, k = j)
    
    # Get Accuracy Score
    knn.accuracy <- mean(knn.mdl == gold.c.test$Binary.PercChange)
    
    # Get Error Score & Input
    knn.error <- 1-knn.accuracy
    knn_error <- append(knn_error, knn.error) 
  }
  # Bind to Data Frame (Creating Five Columns for Five Folds)
  knn_perf_df <- cbind(knn_perf_df, knn_error)
}

# Clean Data Frame
knn_perf_df$KNNmeans <- rowMeans(knn_perf_df[,-1])
colnames(knn_perf_df) <- c("K Parameter", "Fold 1", "Fold 2", "Fold 3", "Fold 4",
                           "Fold 5", "Error Means") 
print(knn_perf_df) 

# Find Minimum Error for K Parameter
knn_min_rn <- which.min(knn_perf_df$`Error Means`)
knn_perf_df[knn_min_rn,] 

### Additional Plots for Presentation ###

# Five Fold CV Error Score Against Number of K Nearest Neighbors 
knn.point <- data.frame(`K Parameter` = 13, `Error Means` = 0.4133)

knn.ffcv.plot <- ggplot(knn_perf_df, aes(x = `K Parameter`, y = `Error Means`))+
         geom_line(color = "navy")+
         labs(x = "Number of K Nearest Neighbors", 
              y = "Misclassification Error Score",
              title = "KNN Five Fold CV Performance")+
  geom_text(data = knn.point, 
            aes(x = K.Parameter, y = Error.Means, label = Error.Means),
             vjust = 1.3, color = "navy", size = 3)
knn.ffcv.plot

ggsave(file = "~/Desktop/knnffcv.png", plot = knn.ffcv.plot, width = 10,
       height = 6, bg = "white")

# Create a Scatter Plot for Fifth Fold Test Points
plot(gold.c.test$PercChangeLag1, gold.c.test$FedFundsRate, 
     col = ifelse(gold.c.test$Binary.PercChange == 1, "salmon","navy"), 
     pch = 1, xlab = "Lagged MoM Percentage Change in Gold Prices", 
     ylab = "Fed Funds Rate", main = "Gold Price Binary Classifier")
legend("bottomright", legend = c("1 as > 0.1%", "0 as < 0.1%"),
       pch = c(1,1), col = c("salmon", "navy"), cex = 0.7)

# Create Scatter Plot with Fifth Fold Test Points & KNN Predictions
plot(gold.c.test$PercChangeLag1, gold.c.test$FedFundsRate, 
     col = ifelse(gold.c.test$Binary.PercChange == 1, "salmon","navy"), 
     pch = 1, xlab = "Lagged MoM Percentage Change in Gold Prices", 
     ylab = "Fed Funds Rate", main = "Gold Price Binary Classifier with KNN Predictions")
points(gold.c.test$PercChangeLag1, gold.c.test$FedFundsRate,
       col = ifelse(knn.mdl == 1, "salmon","navy"), pch = 4) 
legend("bottomright", legend = c("Pred. 1 as > 0.1%", "Pred. 0 as < 0.1%"), 
       pch = c(4,4), col = c("salmon", "navy"), cex = 0.7)
```

Both our train hyper-grid search function and manual grid search show that the optimal number for K parameters (the number of neighboring observations) is 13. In other words, 13 observations closest to our observation of interest will be checked to see whether they identify with class 1 (above 0.1% gold price growth) or class 0 (below 0.1% gold price growth). The majority of these 13 classes will be assigned to the observation of interest. Using this algorithm, we get a mis-classification error (on average) of **41.33%**. This is below 50% and is considered better than random chance. Our manual fit is also similar to running an optimized KNN model on five fold cross validation, so we don't have to run that again. The mis-classification error of 41.33%, thus, is a comparable number to other approaches (as we run five fold cross validation for them). 

### Random Forest & Bagging
```{r}
# Optimize Random Forest Model for Parameters Using Train Function
library(tree)
set.seed(75849)
ctrl <- trainControl(method = "cv", number = 5) 

# Set Grid of Parameters for Random Forests
rf.c.grid <- expand.grid(mtry = 1:22, ntree = c(100,200,300,400,500,1000,1500))
nrow(rf.c.grid)

# Set Up Data Frame to Input Values from Manual Grid Search
rf.perf.df <- data.frame(rf.c.grid[,1]) # First Column is mtry
rf.perf.df <- cbind(rf.perf.df, rf.c.grid[,2]) # Second Column is number of trees

# Manual For Loop Grid Search Since Train Function Refuses to Work
for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.c.train <- gold.c[train_index,]
  gold.c.test <- gold.c[-train_index,] 
  
  # Set Up Empty Vector to Input Misclassificaiton Errors for Each Fold
  rf_error_vec <- vector()
  
  # Now Set Up Grid Search For Loop Using rf.perf.df Parameter Grid
  for (j in 1:nrow(rf.perf.df)){ # Will Havet to Change Just Testing
    # Fit the Model, Mtry is the first column of grid, Ntrees is second column
    rf.model <- randomForest(as.factor(Binary.PercChange) ~.,
                             data = gold.c.train, mtry = rf.perf.df[j,1],
                             ntree = rf.perf.df[j,2])
    
    # Predict on Testing Set, Response is a Class Variable
    rf.predictions <- predict(rf.model, newdata = gold.c.test, type = "class")
    
    # Calculate Accuracy Score
    rf.accuracy <- mean(rf.predictions == gold.c.test$Binary.PercChange)
    
    # Calculate Error Scores and Input into Vector
    rf.error <- 1-rf.accuracy
    rf_error_vec[j] <- rf.error
  }
  
  # Put Error Vector into RF Data Frame to Measure Error Scores
  rf.perf.df <- cbind(rf.perf.df, rf_error_vec)
  
  # Run This Back Up Again for Five Folds, Data Frame Has Five Columns
}

# Clean Up Data Frame
rf.perf.df$Means <- rowMeans(rf.perf.df[,-c(1,2)]) 
colnames(rf.perf.df) <- c("Mtry Parameters", "Number of Trees", "Fold 1", "Fold 2",
                          "Fold 3", "Fold 4", "Fold 5", "FF CV Errors")
print(rf.perf.df) 

# Find Bagging Misclassification Errors
rf.perf.df[rf.perf.df$`Mtry Parameters` == 22,]

# Find Minimum Misclassification Error
rf_min_rn <- which.min(rf.perf.df$`FF CV Errors`)
rf.perf.df[rf_min_rn,] 

### Additional Plots for Presentation ### 

# Variable Importance
rf.mdl.pres.try <- randomForest(as.factor(Binary.PercChange) ~.,
                             data = gold.c, mtry = 18,
                             ntree = 200, importance = TRUE) 
varImpPlot(rf.mdl.pres, cex = 0.55, main = "Random Forest Variable Importance") 

# Grid Plot of MSE Performance through Five Fold
classif.rf.plot <- ggplot(rf.perf.df, aes(x = `Mtry Parameters`, y = `Number of Trees`,size = `FF CV Errors`))+geom_point(col = "navy")+
  labs(x = "Number of Variables Sampled (Mtry)", 
       title = "Classification Random Forest Performance")

ggsave(file = "~/Desktop/classifrfplot.png", plot = classif.rf.plot, width = 10, height = 6, bg = "white")
```

Since our train control function was not working, we decided to run a manual parameter grid search for number of trees (ntrees) and number of features to consider for each tree (mtry). This like the manual knn parameter search is equivalent to testing the random forest model's performance on our five cross validation folds. Thus, the results from this grid search are comparable to other classificaiton models we ran. After running random forests and bagging algorithms on our data set, we find that a random forest with parameters (mtry = 18 and number of trees = 200) is the most optimal in minimizing misclassification errors. It returned a misclassification error of **42.28%** which is slightly higher than our KNN model but still less than 50%. This shows that our model performs better than random chance when it comes to classifying the direction of gold prices month over month, given our 0.1% hurdle rate. Looking at our bagging algorithms (mtry = 22, where we use all predictors in our feature space), their misclassification errors, on average, fell in the range **44.76%-47.42%**. The bagging algorithms were not optimal in our classification setting. 

### Boosting
```{r}
# Conduct Parameter Search for Gradient Boosting Model Using Train Function
set.seed(75849)
ctrl <- trainControl(method = "cv", number = 5) # Five Fold CV

# Create Parameter Grid
parameters.gbm.c <- expand.grid(n.trees = c(100,200,500,1000,1500),
                                interaction.depth = c(5,10,15,20,50,100), 
                                shrinkage = c(0.001,0.01,0.05,0.075,0.1),
                                n.minobsinnode = c(5,10,15,20))

# Fit A Boosting GBM Model for Classification
features.c <- gold.c[, !colnames(gold.c) %in% "Binary.PercChange"]

# trained.gbm.fit.c <- train(x = features.c, y = as.factor(gold.c$Binary.PercChange),
#                            method = "gbm", trControl = ctrl, 
#                            tuneGrid = parameters.gbm.c)

# trained.gbm.fit.c$bestTune

### Train Function Results ###
# Number of Trees: 200
# Interaction Depth: 10
# Shrinkage: 0.075
# Minimum Number of Observations: 15
```

After running a parameter search on our data using the train function, the best tuning parameters for our classification gradient boosting model are 200 for the number of trees, 10 for the interaction depth, 0.075 for the shrinkage parameter, and 15 for the minimum number of observations in a node. We'll use these parameters to calculate the five fold cross validation misclassification error. This will allow us to compare model performance. 

```{r}
# Set Misclassification Error Vector
gbm.c.error <- vector() 

# Five Fold Cross Validation with Best Parameters
for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.c.train <- gold.c[train_index,]
  gold.c.test <- gold.c[-train_index,] 
  
  # Fit the Model with Optimal Parameters Listed Above
  gbm.mdl.c <- gbm(Binary.PercChange ~., data = gold.c.train,
                   distribution = "bernoulli", shrinkage = 0.075,
                   n.trees = 200, interaction.depth = 10, 
                   n.minobsinnode = 15)
  
  # Predict the Training Model on Testing Data
  gbm.predict <- predict(gbm.mdl.c, newdata = gold.c.test, type = "response")
  
  # Convert Response Prediction to Classifier
  gbm.class <- ifelse(gbm.predict > 0.5, 1,0)
  
  # Accuracy Scores
  accuracy <- mean(gbm.class == gold.c.test$Binary.PercChange)
  
  # Error Scores & Input
  err <- 1-accuracy
  gbm.c.error <- append(gbm.c.error,err)
}

# Print Error Vector & Retrieve Five Fold Cross Validation Mean
print(gbm.c.error)
mean(gbm.c.error)
```

After running five fold cross validation on our classification gradient boosting model, we received a mean misclassification error of **43.06%**. This is better than our classification random forest model, but it does not perform as well as our KNN model. Regardless, our misclassification error is below 50% and is better than random chance. 

### Support Vector Machine

For support vector machines, there are two main subsets of models that we'd like to explore in our classification task. The first is running support vector machines with polynomial kernels, and the second is running support vector machines with radial kernels. When running support vector machines with polynomial kernels, we'll vary for cost and degree - the latter represents the maximum polynomial transformation of our predictors. For support vector machines with radial kernels, we'll vary for cost and gamma. We'll use the tune function to run a parameter grid search for cost and gamma/degree. If the tune function does not work, we'll run a for loop to run the grid search. In order to compare our support vector machine results to other classification models, we'll run five fold cross-validation. 

```{r}
### Polynomial Kernel: Tune Function ###
library(e1071)

# Create Polynomial Grid: First Column is Cost, Second Column is Degree
svm.param.c.poly <- expand.grid(cost = c(0.1,0.5,1,2,5,8,10,100),
                                degree = c(2,3,4,5))

# Create Data Frame to Store Misclassification Errors
svm.poly.df <- data.frame(svm.param.c.poly)

# For Loop Grid Search Using Five Fold Cross Validation
for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.c.train <- gold.c[train_index,]
  gold.c.test <- gold.c[-train_index,] 
  
  # Set Error Vector to Input Misclassification Errors for Each Fold
  svm.error.vec <- vector()
  
  # Now Begin Fitting Model with Each Fold, Varying Parameters
  for (j in 1:nrow(svm.poly.df)){
    
    # Fit Model wtih Varying Parameters: First Column = Cost, Second = Degree
    svm.mdl <- svm(as.factor(Binary.PercChange) ~., data = gold.c.train,
                   cost = svm.poly.df[j,1], degree = svm.poly.df[j,2])
    
    # Predictions of Class (Binary Classifier)
    svm.predictions <- predict(svm.mdl, newdata = gold.c.test)
    
    # Accuracy Score
    svm.accuracy <- mean(svm.predictions == gold.c.test$Binary.PercChange)
    
    # Error Score & Input Into Vector
    svm.error <- 1-svm.accuracy
    svm.error.vec[j] <- svm.error
    
    # Input Error into Data Frame in Outermost (Fold) Loop
  }
  
  # Input Error Vector into Data Frame
  svm.poly.df <- cbind(svm.poly.df, svm.error.vec)
  
  # Error Vector Gets Washed Out At Start of New Loop
}

# Clean Data Frame
svm.poly.df$Means <- rowMeans(svm.poly.df[,-c(1,2)])
colnames(svm.poly.df) <- c("Cost Parameter", "Degree Parameter","Fold 1",
                           "Fold 2", "Fold 3", "Fold 4", "Fold 5",
                           "Average Error")
print(svm.poly.df) 

# Find Minimized Misclassification Error and Optimal Parameters for Polynomial Kernel
svm.poly.min <- which(svm.poly.df$`Average Error` == min(svm.poly.df$`Average Error`))
print(svm.poly.df[svm.poly.min,]) 

### Additional Plots for Presentation ###

# Grid Search Plot
svm.plot.poly <- ggplot(svm.poly.df, aes(x = `Cost Parameter`, y = `Degree Parameter`,
                       size = `Average Error`))+
  geom_point(col = "navy")+labs(x = "Cost Parameter", y = "Degree Parameter", 
       title = "Support Vector Machines with Polynomial Kernel Performance")
svm.plot.poly 

ggsave(file = "~/Desktop/svmplotpoly.png", plot = svm.plot.poly, width = 10, height = 6, bg = "white")

print(svm.poly.df)
```

We performed a grid search for the most optimal cost and degree parameters within polynomial kernels and found that the most optimal parameters are a cost value of 100 and a degree value of 2-5. The cost parameter of 100 seemed to be very good at minimizing misclassification error, regardless of polynomial degree. After running five fold cross validation with our grid search, we find that a cost parameter of 100 returned a misclassification error of **42.48%**. The support vector machine model with polynomial kernel outperforms our tree models (gradient boosting, random forests, and bagging) but still underperforms our KNN model. 

```{r}
### Radial Kernel: Grid Search ###

# Create Radial Grid: First Column is Cost, Second Column is Degree
svm.param.c.radial <- expand.grid(cost = c(0.1,0.5,1,2,5,8,10),
                                gamma = c(0.1,0.5,1,2,5,8,10))

# Create Data Frame to Store Misclassification Errors
svm.radial.df <- data.frame(svm.param.c.radial)

# For Loop Grid Search Using Five Fold Cross Validation
for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.c.train <- gold.c[train_index,]
  gold.c.test <- gold.c[-train_index,] 
  
  # Set Error Vector to Input Misclassification Errors for Each Fold
  svm.error.vec <- vector()
  
  # Now Begin Fitting Model with Each Fold, Varying Parameters
  for (j in 1:nrow(svm.radial.df)){
    
    # Fit Model wtih Varying Parameters: First Column = Cost, Second = Degree
    svm.mdl <- svm(as.factor(Binary.PercChange) ~., data = gold.c.train, 
                   kernel = "radial",
                   cost = svm.radial.df[j,1], gamma = svm.radial.df[j,2])
    
    # Predictions of Class (Binary Classifier)
    svm.predictions <- predict(svm.mdl, newdata = gold.c.test)
    
    # Accuracy Score
    svm.accuracy <- mean(svm.predictions == gold.c.test$Binary.PercChange)
    
    # Error Score & Input Into Vector
    svm.error <- 1-svm.accuracy
    svm.error.vec[j] <- svm.error
    
    # Input Error into Data Frame in Outermost (Fold) Loop
  }
  
  # Input Error Vector into Data Frame
  svm.radial.df <- cbind(svm.radial.df, svm.error.vec)
  
  # Error Vector Gets Washed Out At Start of New Loop
}

# Clean Data Frame
svm.radial.df$Means <- rowMeans(svm.radial.df[,-c(1,2)])
colnames(svm.radial.df) <- c("Cost Parameter", "Gamma Parameter","Fold 1",
                           "Fold 2", "Fold 3", "Fold 4", "Fold 5",
                           "Average Error")
print(svm.radial.df) 

# Find Minimized Misclassification Error and Optimal Parameters for Polynomial Kernel
svm.radial.min <- which(svm.radial.df$`Average Error` == min(svm.radial.df$`Average Error`))
print(svm.radial.df[svm.radial.min,]) 

# SVM Plots: Maybe Don't Use This 
svm.plot.radial <- ggplot(svm.radial.df, aes(x = `Cost Parameter`, y = `Gamma Parameter`,
                       size = `Average Error`))+
  geom_point(col = "navy")+labs(x = "Cost Parameter", y = "Gamma Parameter", 
       title = "Support Vector Machines with Radial Kernel Performance")

svm.plot.radial

# Save Plot
ggsave(file = "~/Desktop/svmplotradial.png", plot = svm.plot.radial, width = 10, height = 6, bg = "white")

svm.radial.full <- svm(as.factor(Binary.PercChange) ~., data = gold.c, 
                       kernel = "radial", cost = 1, gamma = 0.1)

plot(svm.radial.full, gold.c, PercChangeLag1 ~ FedFundsRate, 
     xlab = "Fed Funds Rate (Monthly Average)", ylab = "Gold Price Lagged Percent Change")
```

After performing a grid search on cost and gamma parameters for our support vector machine with radial kernels, we find that the most optimal parameter values are a cost parameter of 1 and a gamma parameter of 0.1. These parameter values return a misclassification error of **41.90%**. This edges our polynomial kernels, but it still slightly underperforms our KNN model - though it is the closest among the classification models to the KNN model. 

### Neural Network

Disclaimer: The neural network models ran on one of our markdown files, but can't on this one. In our final presentation, we added our findings from our neural network model. We won't run it in this document because it won't knit. 

```{r}
# #py_install("tensorflow")
# library(keras)
# #reticulate::install_miniconda()
# reticulate::py_install("tensorflow")
# reticulate::py_install("keras")
# 
# # Define the parameter grid
# layers_grid <- c(1, 2, 3)  # Different numbers of layers
# neurons_grid <- c(32, 64, 128)  # Different numbers of neurons
# 
# # Initialize variables to store the best configuration and accuracy
# best_layers <- NULL
# best_neurons <- NULL
# best_accuracy <- 0
# 
# # Perform grid search
# for (layers in layers_grid) {
#   for (neurons in neurons_grid) {
#     # Create the sequential model
#     model <- keras_model_sequential()
#     model %>%
#       layer_dense(units = neurons, activation = "relu", input_shape = ncol(gold.c) - 1)
#     for (i in seq(layers - 1)) {
#       model %>%
#         layer_dense(units = neurons, activation = "relu")
#     }
#     model %>%
#       layer_dense(units = 1, activation = "sigmoid")
# 
#     # Compile the model
#     model %>% compile(
#       loss = "binary_crossentropy",
#       optimizer = "adam",
#       metrics = c("accuracy")
#     )
# 
#     # Train the model
#     history <- model %>% fit(
#       x = as.matrix(gold.c[, -ncol(gold.c)]),
#       y = as.matrix(gold.c$Binary.PercChange),
#       epochs = 10,
#       batch_size = 32,
#       validation_split = 0.2
#     )
# 
#     # Calculate the accuracy
#     accuracy <- history$metrics$val_accuracy[length(history$metrics$val_accuracy)]
# 
#     # Check if the current configuration is the best so far
#     if (accuracy > best_accuracy) {
#       best_layers <- layers
#       best_neurons <- neurons
#       best_accuracy <- accuracy
#       best_history <- history
# 
#     }
#   }
# }
# 
# # Print the best configuration and accuracy
# print(paste("Best Layers:", best_layers))
# print(paste("Best Neurons:", best_neurons))
# print(paste("Best Accuracy:", best_accuracy))
# 
# # Plot the training history of the best model
# plot(best_history$metrics$accuracy, type = "l", col = "blue", xlab = "Epoch", ylab = "Accuracy",
#      main = "Training History - Best Model")
# lines(best_history$metrics$val_accuracy, col = "red")
# legend("bottomright", legend = c("Training Accuracy", "Validation Accuracy"), col = c("blue", "red"), lty = 1)
# 
# 
# library(pROC)
# 
# # Predict probabilities
# y_pred <- model %>% predict(as.matrix(gold.c[, -ncol(gold.c)]))
# 
# # Compute ROC curve
# roc_data <- roc(gold.c$Binary.PercChange, y_pred)
# 
# # Plot ROC curve
# plot(roc_data, main = "ROC Curve", print.auc = TRUE)

```


### ROC Graph for Classification Models
```{r}
# Data Frame for Predicted Probability Values & Actual Values
empty_vector <- 1:nrow(gold.c)
roc.df <- data.frame(empty_vector)

# Vectors for Predicted Models/Actual observations: Run This Before Each Fold
gold.c.classes <- vector()
lda.pred.vector <- vector()
qda.pred.vector <- vector()
knn.pred.vector <- vector()
rf.pred.vector <- vector()
boost.pred.vector <- vector()
svm.pred.vector <- vector()

# Fitting the Models with Five Folds
for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.c.train <- gold.c[train_index,]
  gold.c.test <- gold.c[-train_index,]  
  
  # Append Actual Observations to a Vector
  gold.c.classes <- append(gold.c.classes, gold.c.test$Binary.PercChange)
  
  # Set Inputs for KNN Function With Each Fold
  train_features <- gold.c.train[,-length(gold.c.train)]
  test_features <- gold.c.test[,-length(gold.c.train)]
  train_class <- gold.c.train$Binary.PercChange
  qdaFeatures <- c("PercChangeLag1", "Indus.Prod.Ind", "FedFundsRate")
  
  # Fit Various Models to Folds 
  knn.mdl <- knn(train_features, test_features, train_class, k = 13, prob = TRUE)
  rf.model <- randomForest(as.factor(Binary.PercChange) ~.,
                             data = gold.c.train, mtry = 18,
                             ntree = 200)
  gbm.mdl.c <- gbm(Binary.PercChange ~., data = gold.c.train,
                   distribution = "bernoulli", shrinkage = 0.075,
                   n.trees = 200, interaction.depth = 10, 
                   n.minobsinnode = 15)
  svm.mdl <- svm(as.factor(Binary.PercChange) ~., data = gold.c.train,
                 kernel = "radial", cost = 1, gamma = 0.1, prob = TRUE) 
  ldaModel <- lda(as.factor(Binary.PercChange) ~ ., data = gold.c.train)
  qdaModel <- qda(as.factor(Binary.PercChange) ~ ., data =
                    gold.c.train[,c(qdaFeatures, "Binary.PercChange")])
  
  
  # Predict on Various Models
  knn.pred <- attr(knn.mdl, "prob")
  rf.pred <- predict(rf.model, gold.c.test, type = "prob")
  gbm.pred <- predict(gbm.mdl.c, gold.c.test, type = "response")
  svm.pred <- predict(svm.mdl, gold.c.test, prob = TRUE)
  ldaPredictions <- predict(ldaModel, newdata = gold.c.test)$posterior[,2]
  qdaPredictions <- predict(qdaModel, newdata = gold.c.test[, qdaFeatures])$posterior[,2]
  
  # Append Predictions to Predict Vectors
  knn.pred.vector <- append(knn.pred.vector, knn.pred) 
  rf.pred.vector <- append(rf.pred.vector, rf.pred[,2])
  boost.pred.vector <- append(boost.pred.vector, gbm.pred)
  svm.pred.vector <- append(svm.pred.vector, svm.pred)
  lda.pred.vector <- append(lda.pred.vector, ldaPredictions)
  qda.pred.vector <- append(qda.pred.vector, qdaPredictions)
  
}
 
# ROC Curves
library(ROCR)
roc_knn <- pROC::roc(gold.c.classes, knn.pred.vector)
roc_rf <- pROC::roc(gold.c.classes, rf.pred.vector)
roc_gbm <- pROC::roc(gold.c.classes, boost.pred.vector)
roc_svm <- pROC::roc(gold.c.classes, svm.pred.vector)
roc_lda <- pROC::roc(gold.c.classes, lda.pred.vector)
roc_qda <- pROC::roc(gold.c.classes, qda.pred.vector)

# Individual Plots
plot(roc_knn, main = "ROC Curve: KNN", print.auc = TRUE, col = "navy") 
plot(roc_rf, main = "ROC Curve: Random Forest", print.auc = TRUE, col = "navy")
plot(roc_gbm, main = "ROC Curve: GBM", print.auc = TRUE, col = "navy")
plot(roc_svm, main = "ROC Curve: SVM w. Radial Kernel", print.auc = TRUE, col = "navy")
plot(roc_lda, main = "ROC Curve: LDA Curve", print.auc = TRUE, col = "navy")
plot(roc_qda, main = "ROC Curve: QDA Curve", print.auc = TRUE, col = "navy")

# LDA/QDA
plot(roc_lda, main = "ROC Curve: LDA & QDA", print.auc = TRUE, col = "navy")
lines(roc_qda, col = "salmon")
legend("bottomright", legend = c("LDA: 0.605", "QDA: 0.593"),
       col = c("navy","salmon"),  lwd = 2, cex = 0.6,
       lty = c(1, 1), title = "Lines & AUC")

# All Together Now
plot(roc_knn, main = "ROC Curve: All Classification Models", col = "navy") 
lines(roc_rf, col = "magenta")
lines(roc_gbm, col = "green")
lines(roc_svm, col = "brown") 
lines(roc_lda, col = "orange")
lines(roc_qda, col = "cyan")
legend("bottomright", legend = c("KNN: 0.503", "RF: 0.567","GBM: 0.567",
                                 "SVM: 0.562", "LDA: 0.518", "QDA: 0.593"), # Put AUCs in here
       col = c("navy", "magenta","green","brown","orange","cyan"), 
       lwd = 2, cex = 0.6,
       lty = c(1, 1), title = "Lines & AUC")

pROC::auc(roc_knn) 
pROC::auc(roc_rf) 
pROC::auc(roc_gbm) 
pROC::auc(roc_svm) 
```










