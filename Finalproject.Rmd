---
title: "Final Project"
author: "Shanie Talor, Arif Abd Aziz, Jon Girolamo"
date: '2023-06-02'
output: pdf_document
---

# Introduction

(We don't have to polish this RMD in terms of labels or descriptions. Final presentation will be in powerpoint. We'll still submit this as the source code though so we should still be commenting what we're doing with code) Jon

# Load Data
```{r}
# Read csv
gold <- read.csv("gold.csv", sep = ",", header = TRUE)

sentiment <- read.csv("sentiment.csv", sep = ",", header = TRUE)
sentiment <- sentiment[-c(1:9, 543, 544), ]

gold$sentiment <- sentiment$sentiment

head(gold)

# Omit all rows that have NA Values: 6 Rows: 05/31/1979 to 03/31/2023
gold <- na.omit(gold)

# Add Binary Classifier Variable
gold$Binary.PercChange <- ifelse(gold$PercChangeForc > 0.1, 1, 0)

# Check if Features Are Numeric (Returns False, So Non-Numeric Variables)
all(sapply(gold, is.numeric))


# gold <- gold[,-26]
# gold <- gold[,-24]
# gold <- gold[,-2]
# gold <- gold[,-1]

# head(gold)
# Head Gold
head(gold)
```

We converted our percent change forecast variable into a binary classifier. The hurdle rate for our binary classifier ("Binary.PercChange") is set at 0.1%, so that edge cases that are very close to 0% (0%:0.0999%) are classified as non-investment opportunities. Percent changes in gold (month over month) greater than 0.1% will be considered "buy" opportunities, with a classifier value of 1. 

# Initial Analysis of Features 

In order to better understand our features, we'll first look at the histograms of our features as well as the scatterplots between our features and our main response variables: monthly percent changes in gold for our regression problem and our binary classiifer for our classification problem. 

### Histogram for Loops
```{r}
# Set Feature Space Excluding Lagged Variables and Other Miscellenous Feature 
library(dplyr)
library(ggplot2)
library(stats)
library(reshape2)
gold_histograms <- gold %>% select(-c("Date","PercChangeLag1","PercChangeLag2",
                                "PercChangeLag3", "PercChangeLag4",
                                "PercChangeLag5","Inf.L1","Inf.L2",
                                "Inf.L3","Inf.L4","FedFundsRateL1"))
gold_histograms <- as.data.frame(lapply(gold_histograms, as.numeric))
head(gold_histograms)

# For Loop for Histograms
for (feature in names(gold_histograms)){
  hist(gold_histograms[[feature]], xlab = paste0(feature), ylab = "Frequency",
       main = paste0("Histogram of ",feature))
}

# For Loop for Box Plots
for (feature in names(gold_histograms)){
  boxplot(gold_histograms[[feature]], ylab = paste0("Value of ",feature),
       main = paste0("Boxplot of ",feature))
}
```

The boxplots and histograms show that our variables are on average normally distributed. There are outliers in some of our features, such as percent changes in monthly prices and changes in bank reserves. However, we believe these outliers are still essential in predicting the movement of gold prices. 

### Correlation Values
```{r} 
# Data Clean for Scatter Plots 
set.seed(490782)
gold_cor <- gold %>% select(-c("Date","Average.Price")) 
gold_cor <- as.data.frame(lapply(gold_cor, as.numeric))
head(gold_cor)

# For Loop for Scatter Plots 
for (feature in names(gold_cor)[!names(gold_cor) %in% c("Binary.PercChange")]){
  plot(gold_cor[[feature]], gold_cor$PercChangeForc, type = "p",
       xlab = paste(feature), ylab = "Monthly Forecasted Change in Gold Prices",
       main = paste0("Monthly Changes in Gold Prices Against ",feature)) 
  line <- lm(gold_cor$PercChangeForc ~ gold_cor[[feature]])
  abline(line, col = "blue")
}

# Heat Map
cor_matrix <- round(cor(gold_cor),2)
melted_cor_gold <- melt(cor_matrix)
ggplot(data = melted_cor_gold, aes(x = Var1, y = Var2, fill = value))+
  geom_tile()+theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Our scatterplots and heat maps show that there are some pockets of correlation between features in our data set. Looking at our main response variables (PercChangeForc & Binary.PercChange), there are no visibly strong correlations between them and our various features. However, we believe a combination and enhancement of these features through the various algorithms we'll be looking at will allow us to best predict both response variables - in a regression and classification setting. 

# Initial Data Cleaning
```{r}
# Gold Data for Regression Problem:
head(gold)
gold.r <- gold %>% select(-c("Date","Average.Price","Binary.PercChange","Oil",
                             "PPIJewelry"))

# Gold Data for Classification Problem:
gold.c <- gold %>% select(-c("Date","Average.Price","PercChangeForc","Oil",
                             "PPIJewelry")) 

# Convert Data to Numeric
gold.r <- as.data.frame(lapply(gold.r, as.numeric)) 
gold.c <- as.data.frame(lapply(gold.c, as.numeric))

# Data Check & Numeric Check
all(sapply(gold.r, is.numeric))
all(sapply(gold.c, is.numeric))
head(gold.r)
head(gold.c)
```

# Feature Selection: Boruta Algorithm
```{r, echo = FALSE}
# Run Boruta  algorithm on clean data
library(Boruta)
library(randomForest)
boruta.gold <- Boruta(PercChangeForc~., data = gold.r, doTrace = 2, 
                       randomForest = TRUE)
```

### Plot Boruta Algorithm
```{r, echo = FALSE}
plot(boruta.gold, main = "Boruta Algorithm for Gold Pricing Algorithm")
```

# Create Five Fold Cross Validation
```{r}
library(caret)
set.seed(123)
folds <- createFolds(gold.r$PercChangeForc, k = 5, returnTrain = TRUE)
```

# Regression

### OLS (Jon)
```{r}
set.seed(123)
# Run linear regression
lin.reg <- lm(PercChangeForc~., data = gold.r)

summary(lin.reg)

# Create empty vector
lin.reg.mse <- vector()

# CV loop for linear regression
for(i in 1:5){
  
  training.i <- folds[[i]]
  testing.i <- setdiff(1:length(gold.r$PercChangeForc), training.i)
  gold.test <- gold.r[testing.i,]
  gold.train <- gold.r[training.i,]

  # Model
  lin.reg <- lm(PercChangeForc~., data = gold.train)
  
  # Model pred
  lin.reg.pred <- predict(lin.reg, newdata = gold.test)
  
  # Store MSE vals
  lin.reg.mse[i] <- mean((lin.reg.pred-gold.test$PercChangeForc)^2)
  
}

lin.reg.mse
mean(lin.reg.mse)

```

### Ridge & Lasso (Shanie)

### Principal Components Regression (Arif)

### Random Forest + Bagging (Jon)
```{r}
library(gbm)
set.seed(123)

mtry.vals <- seq(1, 22, by = 1)

bag.mse <- vector()
mod.df.r <- data.frame(mtry.vals)

for (i in 1:5){
  
  training.i <- folds[[i]]
  testing.i <- setdiff(1:length(gold.r$PercChangeForc), training.i)
  gold.test <- gold.r[testing.i,]
  gold.train <- gold.r[training.i,]
  
  
  for (j in seq_along(mtry.vals)){

  mod <- randomForest(PercChangeForc~., data = gold.train, mtry = mtry.vals[j], importance = TRUE)

  mod.pred <- predict(mod, newdata = gold.test)

  bag.mse[j] <- mean((mod.pred-gold.test$PercChangeForc)^2)
  
  }
  # Append our MSE Vector into Our Data Frame (Memory)
  mod.df.r <- cbind(mod.df.r, bag.mse)
  
}
mod.df.r

mod.df.r$mean_mse <- rowMeans(mod.df.r[, -1])

# Optimal bagging mtry
opt.mtry <- mod.df.r[which.min(mod.df.r$mean_mse), ]
opt.mtry



# Optimal Random Forest MSE: mtry = 1, MSE = 21.21

# Bagging MSE: MSE = 25.70529

```

### Boosting (Jon)
```{r}
set.seed(123)
library(caret)
library(gbm)

# # Train Function with Grid of Parameters
# control <- trainControl(method = "cv", number = 5)
# 
# # Create Parameter Grid
# parameters.r <- expand.grid(n.trees = c(100,200,500,1000,1500),
#                           interaction.depth = c(5,10,15,20,50,100),
#                           shrinkage = c(0,0.001,0.005,0.01,0.03,0.05),
#                           n.minobsinnode = c(5))
# 
# # Fit A Boosting (GBM) Model 
# features.r <- gold.r[, !colnames(gold.r) %in% "PercChangeForc"]
# 
# trained.gbm.fit.r <- train(x = features.r, y = gold.r$PercChangeForc, 
#                            method = "gbm", trControl = control, 
#                            tuneGrid = parameters.r)
# 
# trained.gbm.fit.r$bestTune

# Output:

# n.trees = 100	
# 
# interaction.depth = 5
# 
# shrinkage = 0.01
# 
# n.minobsinnode = 5




# Run Boosted Model with optimal Parameters to calculate CV MSE:

boost.mse <- vector()

for (i in 1:5){

  training.i <- folds[[i]]
  testing.i <- setdiff(1:length(gold.r$PercChangeForc), training.i)
  gold.test <- gold.r[testing.i,]
  gold.train <- gold.r[training.i,]
  
  gold.r.boost <- gbm(PercChangeForc ~ ., data = gold.train, distribution = "gaussian", 
                   n.trees = 100, interaction.depth = 5,
                   shrinkage = 0.01)

  # Made preds
  yhat <- predict(gold.r.boost, newdata = gold.test, n.trees = 100)

  # MSE calc
  boost.mse[i] <- mean((yhat - gold.test$PercChangeForc)^2)
}

boost.mse
mean(boost.mse)

```


### Neural Network
```{r}

```


--------------------------------------------------------------------------------

# Classificatio Problem

### LDA (Shanie)
```{r}

```

### QDA (Shanie)
```{r}

```

### KNN: Using Optimal Train (Arif)
```{r}
# Optimize Model for Best K Parameters Using Train Function 
library(class)
set.seed(75849)
ctrl <- trainControl(method = "cv", number = 5) # Five Fold CV

# Parameter Grid Search
k_values <- expand.grid(k = 1:100)

# Train the Model on Grid Set Above - Parameter Grid Search
knn_optimize <- train(as.factor(Binary.PercChange) ~., data = gold.c, 
                      method = "knn", trControl = ctrl, tuneGrid = k_values)
knn_optimize$bestTune

# Optimize Models (More Manually)
knn_vector_value <- seq(3,51,2)
knn_perf_df <- data.frame(knn_vector_value)

for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.c.train <- gold.c[train_index,]
  gold.c.test <- gold.c[-train_index,] 
  
  # Set Inputs for KNN Function With Each Fold
  train_features <- gold.c.train[,-length(gold.c.train)]
  test_features <- gold.c.test[,-length(gold.c.train)]
  train_class <- gold.c.train$Binary.PercChange
  
  # Set Up Vector To Input Error Scores
  knn_error <- vector() # This vector will be wiped out with each fold run
  
  # Set Up Inner For Loop for KNN Factors, 3-51 (That Would be 50 Parameters) 
  for (j in seq(3,51,2)){
    # Run Model
    knn.mdl <- knn(train_features, test_features, train_class, k = j)
    
    # Get Accuracy Score
    knn.accuracy <- mean(knn.mdl == gold.c.test$Binary.PercChange)
    
    # Get Error Score & Input
    knn.error <- 1-knn.accuracy
    knn_error <- append(knn_error, knn.error) 
  }
  # Bind to Data Frame (Creating Five Columns for Five Folds)
  knn_perf_df <- cbind(knn_perf_df, knn_error)
}

# Clean Data Frame
knn_perf_df$KNNmeans <- rowMeans(knn_perf_df[,-1])
colnames(knn_perf_df) <- c("K Parameter", "Fold 1", "Fold 2", "Fold 3", "Fold 4",
                           "Fold 5", "Error Means") 
print(knn_perf_df) 

# Find Minimum Error for K Parameter
knn_min_rn <- which.min(knn_perf_df$`Error Means`)
knn_perf_df[knn_min_rn,]
```

Both our train hyper-grid search function and manual grid search show that the optimal number for K parameters (the number of neighboring observations) is 13. In other words, 13 observations closest to our observation of interest will be checked to see whether they identify with class 1 (above 0.1% gold price growth) or class 0 (below 0.1% gold price growth). The majority of these 13 classes will be assigned to the observation of interest. Using this algorithm, we get a mis-classification error (on average) of **41.33%**. This is below 50% and is considered better than random chance. Our manual fit is also similar to running an optimized KNN model on five fold cross validation, so we don't have to run that again. The mis-classification error of 41.33%, thus, is a comparable number to other approaches (as we run five fold cross validation for them). 

### Random Forest & Bagging (Arif)
```{r}
# Optimize Random Forest Model for Parameters Using Train Function
library(tree)
set.seed(75849)
ctrl <- trainControl(method = "cv", number = 5) 

# Set Grid of Parameters for Random Forests
rf.c.grid <- expand.grid(mtry = 1:22, ntree = c(100,200,300,400,500,1000,1500))
nrow(rf.c.grid)

# Set Up Data Frame to Input Values from Manual Grid Search
rf.perf.df <- data.frame(rf.c.grid[,1]) # First Column is mtry
rf.perf.df <- cbind(rf.perf.df, rf.c.grid[,2]) # Second Column is number of trees

# Manual For Loop Grid Search Since Train Function Refuses to Work
for (i in 1:length(folds)){
  # Set Folds
  train_index <- folds[[i]]
  gold.c.train <- gold.c[train_index,]
  gold.c.test <- gold.c[-train_index,] 
  
  # Set Up Empty Vector to Input Misclassificaiton Errors for Each Fold
  rf_error_vec <- vector()
  
  # Now Set Up Grid Search For Loop Using rf.perf.df Parameter Grid
  for (j in 1:nrow(rf.perf.df)){ # Will Havet to Change Just Testing
    # Fit the Model, Mtry is the first column of grid, Ntrees is second column
    rf.model <- randomForest(as.factor(Binary.PercChange) ~.,
                             data = gold.c.train, mtry = rf.perf.df[j,1],
                             ntree = rf.perf.df[j,2])
    
    # Predict on Testing Set, Response is a Class Variable
    rf.predictions <- predict(rf.model, newdata = gold.c.test, type = "class")
    
    # Calculate Accuracy Score
    rf.accuracy <- mean(rf.predictions == gold.c.test$Binary.PercChange)
    
    # Calculate Error Scores and Input into Vector
    rf.error <- 1-rf.accuracy
    rf_error_vec[j] <- rf.error
  }
  
  # Put Error Vector into RF Data Frame to Measure Error Scores
  rf.perf.df <- cbind(rf.perf.df, rf_error_vec)
  
  # Run This Back Up Again for Five Folds, Data Frame Has Five Columns
}

# Clean Up Data Frame
rf.perf.df$Means <- rowMeans(rf.perf.df[,-c(1,2)]) 
colnames(rf.perf.df) <- c("Mtry Parameters", "Number of Trees", "Fold 1", "Fold 2",
                          "Fold 3", "Fold 4", "Fold 5", "Error Means")
print(rf.perf.df) 

# Find Bagging Misclassification Errors
rf.perf.df[rf.perf.df$`Mtry Parameters` == 22,]

# Find Minimum Misclassification Error
rf_min_rn <- which.min(rf.perf.df$`Error Means`)
rf.perf.df[rf_min_rn,]

# NOT NEEDED ####
# nrow(rf.perf.df)
# # Perform Grid Search
# rf.opt.model <- train(as.factor(Binary.PercChange)~.,
#                       data = gold.c, method = "rf", 
#                     trControl = ctrl, 
#                     tuneGrid = rf.c.grid)
# 
# #Fit Classification Tree
# class.tree.test <- tree(as.factor(Binary.PercChange)~., gold.c)
# summary(class.tree.test)
# 
# plot(class.tree.test)
# text(class.tree.test, pretty=0, cex=0.6)
```

Since our train control function was not working, we decided to run a manual parameter grid search for number of trees (ntrees) and number of features to consider for each tree (mtry). This like the manual knn parameter search is equivalent to testing the random forest model's performance on our five cross validation folds. Thus, the results from this grid search are comparable to other classificaiton models we ran. After running random forests and bagging algorithms on our data set, we find that a random forest with parameters (mtry = 9 and number of trees = 300) is the most optimal in minimizing misclassification errors. It returned a misclassification error of **44.76%** which is slightly higher than our KNN model but still less than 50%. This shows that our model performs better than random chance when it comes to classifying the direction of gold prices month over month, given our 0.1% hurdle rate. Looking at our bagging algorithms (mtry = 22, where we use all predictors in our feature space), their misclassification errors, on average, fell in the range **44.57%-46.10*%**. The bagging algorithms were not optimal in our classification setting. 

### Boosting (Arif)
```{r}

```


### Support Vector Machine (Arif)
```{r}

```



### Neural Network
```{r}

```









